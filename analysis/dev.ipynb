{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib  import cm\n",
    "import matplotlib.animation\n",
    "import matplotlib.gridspec as gridspec\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "%matplotlib ipympl\n",
    "import pims\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from scipy.spatial import KDTree, cKDTree, Voronoi, voronoi_plot_2d, ConvexHull\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "import joblib\n",
    "import time\n",
    "from numba_progress import ProgressBar\n",
    "from tqdm import tqdm\n",
    "import trackpy as tp\n",
    "from numba import njit, prange\n",
    "\n",
    "from yupi import Trajectory\n",
    "import yupi.graphics as yg\n",
    "import yupi.stats as ys\n",
    "\n",
    "from utility import get_imsd, get_imsd_windowed, get_emsd, get_emsd_windowed, fit_hist, MB_2D,\\\n",
    "                    normal_distr, lorentzian_distr, get_trajs, speed_windowed, theta_windowed, \\\n",
    "                    get_smooth_trajs, get_velocities\n",
    "\n",
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "import networkx as nx \n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import graph_tool.all as gt\n",
    "\n",
    "show_verb = False\n",
    "save_verb = True\n",
    "anim_show_verb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Import data...\")\n",
    "if 1:\n",
    "    data_path = \"../tracking/49b_1r/49b_1r_pre_merge/df_linked.parquet\"\n",
    "    res_path = \"./49b_1r/results\"\n",
    "    analysis_data_path = \"./49b_1r/analysis_data\"\n",
    "    red_particle_idx = np.array([8]).astype(int)\n",
    "    fps = 10\n",
    "    maxLagtime = 100*fps # maximum lagtime to be considered in the analysis, 100 seconds\n",
    "    v_step = 10\n",
    "else:\n",
    "    data_path = \"../tracking/25b_25r/df_linked.parquet\"\n",
    "    res_path = \"./25b_25r/results\"\n",
    "    analysis_data_path = \"./25b_25r/analysis_data\"\n",
    "    red_particle_idx = np.sort(np.array([27, 24, 8, 16, 21, 10, 49, 14, 12, 9, 7, 37, 36, 40, 45, 42, 13, 20, 26, 2, 39, 5, 11, 22, 44])).astype(int)\n",
    "    fps = 30\n",
    "    maxLagtime = 100*fps # maximum lagtime to be considered in the analysis, 100 seconds\n",
    "    v_step = 30\n",
    "\n",
    "rawTrajs = pd.read_parquet(data_path)\n",
    "nDrops = int(len(rawTrajs.loc[rawTrajs.frame==0]))\n",
    "red_mask = np.zeros(nDrops, dtype=bool)\n",
    "red_mask[red_particle_idx] = True\n",
    "colors = np.array(['b' for i in range(nDrops)])\n",
    "colors[red_particle_idx] = 'r'\n",
    "\n",
    "frames = rawTrajs.frame.unique().astype(int)\n",
    "nFrames = len(frames)\n",
    "print(f\"Number of Droplets: {nDrops}\")\n",
    "print(f\"Number of Frames: {nFrames} at {fps} fps --> {nFrames/fps:.2f} s\")\n",
    "\n",
    "# ANALYSIS PARAMETERS\n",
    "pxDimension = 1 # has to be defined \n",
    "\n",
    "x = np.arange(1, maxLagtime/fps + 1/fps, 1/fps) # range of power law fit\n",
    "\n",
    "# WINDOWED ANALYSIS PARAMETERS\n",
    "window = 300*fps # 320 s\n",
    "stride = 10*fps # 10 s\n",
    "print(\"Windowed analysis args:\")\n",
    "startFrames = np.arange(0, nFrames-window, stride, dtype=int)\n",
    "endFrames = startFrames + window\n",
    "nSteps = len(startFrames)\n",
    "print(f\"window of {window/fps} s, stride of {stride/fps} s --> {nSteps} steps\")\n",
    "units = \"px/s\"\n",
    "default_kwargs_blue = {\"color\": \"#00FFFF\", \"ec\": (0, 0, 0, 0.6), \"density\": True}\n",
    "default_kwargs_red = {\"color\": \"#EE4B2B\", \"ec\": (0, 0, 0, 0.6), \"density\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = rawTrajs.loc[rawTrajs.frame == 0, ['x', 'y']].values\n",
    "r = rawTrajs.loc[rawTrajs.frame == 0, 'r'].values\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(ref[0])\n",
    "for i, ((x,y),) in enumerate(zip(xy)):\n",
    "    ax.text(x, y, i, ha=\"center\", va=\"center\")\n",
    "    ax.add_artist(plt.Circle((x,y), r[i], color=colors[i], fill=False))\n",
    "ax.set(xlim=(0, ref[0].shape[0]), ylim=(0, ref[0].shape[1]), xlabel=\"X [px]\", ylabel=\"Y [px]\", title = \"Red droplets id identification\")\n",
    "plt.savefig(f\"./{res_path}/initial_frame.png\", bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# check the number of droplets per frame\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(rawTrajs.groupby(\"frame\").size(), color = \"k\", label = \"n of droplets\")\n",
    "ax.axhline(nDrops, color = \"r\", linestyle = \"--\", label = \"correct n\")\n",
    "ax.set(xlabel = \"frame\", ylabel = \"n\")\n",
    "ax.set_title(\"Number of droplets per frame\")\n",
    "ax.grid(linewidth = 0.2)\n",
    "ax.legend()\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# velocity distribution functional analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n Global speed distribution analysis\")\n",
    "blueTrajs, redTrajs = get_trajs(nDrops, red_particle_idx, rawTrajs)\n",
    "bin_borders = np.arange(0, 100, .2)\n",
    "bin_centers = np.arange(0, 100, .2)[:-1] + .2 / 2\n",
    "x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 10000)\n",
    "\n",
    "v_blue = ys.speed_ensemble(blueTrajs, step = v_step)\n",
    "v_red = ys.speed_ensemble(redTrajs, step = v_step)\n",
    "T_blue, T_blue_std = fit_hist(v_blue, bin_borders, MB_2D, [1.])\n",
    "T_red, T_red_std = fit_hist(v_red, bin_borders, MB_2D, [1.])\n",
    "print(\"Trajectories\")\n",
    "print(f\"Blue Particles σ: {T_blue[0]} ± {T_blue_std[0]}\")\n",
    "print(f\"Red Particle σ: {T_red[0]} ± {T_red_std[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_distr = np.histogram(v_blue, bins = bin_borders, density = True)[0]\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(bin_centers, -(2*T_blue[0]**2)*np.log((T_blue[0]**2/bin_centers)*v_distr))\n",
    "#ax.plot(bin_centers, bin_centers**2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spectral velocity analysis ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajs = []\n",
    "for i in range(0, 50):\n",
    "    p = rawTrajs.loc[rawTrajs.particle == i, [\"x\",\"y\"]]\n",
    "    trajs.append(Trajectory(p.x, p.y, dt = 1/10, traj_id=i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = ys.speed_ensemble(trajs, step = 10)\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.hist(v, bins=1000, density=True, color=\"#00FFFF\", ec=(0, 0, 0, 0.6))\n",
    "ax.set_xlabel(\"Speed [px/s]\")\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_title(\"Speed Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = np.abs(np.fft.fft(v))**2\n",
    "time_step = 1 / 30\n",
    "freqs = np.fft.fftfreq(v.size, time_step)\n",
    "idx = np.argsort(freqs)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.plot(freqs[idx], ps[idx])\n",
    "ax.set_xlabel(\"Frequency [Hz]\")\n",
    "ax.set_ylabel(\"Power Spectrum\")\n",
    "ax.set_title(\"Power Spectrum of the speed of the red droplets\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPH ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "factorList = np.round(np.arange(1.4, 3, 0.1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 25000\n",
    "X = np.array(rawTrajs.loc[rawTrajs.frame == frame, ['x', 'y']])\n",
    "# create dictionary with positions of the agents\n",
    "dicts = {}\n",
    "for i in range(len(X)):\n",
    "    dicts[i] = (X[i, 0], X[i, 1])\n",
    "# generate random geometric graph with cutoff distance 2.2 times the mean diameter the droplets have at that frame\n",
    "G = nx.random_geometric_graph(len(dicts), mean_d[frame]*2.2, pos=dicts, dim=2)\n",
    "node_pos = nx.get_node_attributes(G, 'pos')\n",
    "vor = Voronoi(np.asarray(list(node_pos.values())))\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "voronoi_plot_2d(vor, ax=ax, show_vertices=False, line_colors='orange', line_width=2, line_alpha=0.6, point_size=2)\n",
    "nx.draw(G, pos=node_pos, node_size=10, node_color=colors, with_labels=True, ax=ax)\n",
    "ax.set(xlim=(0, 900), ylim=(900, 0), title=f\"frame {frame}\", xlabel=\"x [px]\", ylabel=\"y [px]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHOICE OF CUTOFF DISTANCE IN RANDOM GEOMETRIC GRAPH\n",
    "clustering = np.zeros((len(factorList), len(frames)))\n",
    "n_cycles = np.zeros((len(factorList), len(frames)), dtype=int)\n",
    "dim_cycles_list = []\n",
    "n_conn_comp = np.zeros((len(factorList), len(frames)), dtype=int)\n",
    "conn_comp_list = []\n",
    "progress_bar = tqdm(total=len(frames)*10)\n",
    "\n",
    "for ind, factor in enumerate(factorList):\n",
    "    connected_components = []\n",
    "    dim_cycles = []\n",
    "    for frame in frames:\n",
    "        X = np.array(rawTrajs.loc[rawTrajs.frame == frame, ['x', 'y']])\n",
    "        dicts = {}\n",
    "        for j in range(len(X)):\n",
    "            dicts[j] = (X[j, 0], X[j, 1])\n",
    "        \n",
    "        temp = nx.random_geometric_graph(len(dicts), mean_d[frame]*factor, pos=dicts, dim=2)\n",
    "        clustering[ind, frame] = np.mean(list(nx.clustering(temp).values()))\n",
    "\n",
    "        cycles = nx.cycle_basis(temp)\n",
    "        n_cycles[ind, frame] = int(len(cycles))\n",
    "        dim_cycles.append([len(cycles[i]) for i in range(n_cycles[ind, frame])])\n",
    "\n",
    "        conn_comp = [len(c) for c in sorted(nx.connected_components(temp), key=len, reverse=True)]\n",
    "        n_conn_comp[ind, frame] = len(conn_comp)\n",
    "        connected_components.append(conn_comp)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    conn_comp_list.append(connected_components)\n",
    "    dim_cycles_list.append(dim_cycles)\n",
    "progress_bar.close()\n",
    "\n",
    "mean_dim_conn_comp = np.zeros((len(factorList), len(frames)))\n",
    "for frame in frames:\n",
    "    mean_dim_conn_comp[:, frame] = [np.mean(conn_comp_list[i][frame]) for i in range(10)]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "imshow = ax.imshow(n_conn_comp, aspect='auto', extent=[0, len(frames), factorList[-1], factorList[0]], cmap='viridis')\n",
    "fig.colorbar(imshow, ax=ax)\n",
    "ax.set(xlabel=\"frame\", ylabel=\"factor\", title=\"Number of connected components\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "imshow = ax.imshow(n_cycles, aspect='auto', extent=[0, len(frames), factorList[-1], factorList[0]], cmap='viridis')\n",
    "fig.colorbar(imshow, ax=ax)\n",
    "ax.set(xlabel=\"frame\", ylabel=\"factor\", title=\"Number of cycles\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "imshow = ax.imshow(clustering, aspect='auto', extent=[0, len(frames), factorList[-1], factorList[0]], cmap='viridis')\n",
    "fig.colorbar(imshow, ax=ax)\n",
    "ax.set(xlabel=\"frame\", ylabel=\"factor\", title=\"Clustering coefficient\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "imshow = ax.imshow(mean_dim_conn_comp, aspect='auto', extent=[0, len(frames), factorList[-1], factorList[0]], cmap='viridis')\n",
    "fig.colorbar(imshow, ax=ax)\n",
    "ax.set(xlabel=\"frame\", ylabel=\"factor\", title=\"Mean dimension of connected components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1.6\n",
    "if 0:\n",
    "    clust_id_list = []\n",
    "    for frame in tqdm(frames):\n",
    "        X = np.array(rawTrajs.loc[rawTrajs.frame == frame, ['x', 'y']])\n",
    "        dicts = {}\n",
    "        for j in range(len(X)):\n",
    "            dicts[j] = (X[j, 0], X[j, 1])\n",
    "        temp = nx.random_geometric_graph(len(dicts), mean_d[frame]*factor, pos=dicts, dim=2)\n",
    "        clust_id = np.ones(len(X), dtype=int)*-1\n",
    "        for id, c in enumerate(list(nx.connected_components(temp))):\n",
    "            clust_id[list(c)] = id\n",
    "        clust_id_list += list(clust_id)\n",
    "    clustered_trajs = rawTrajs.copy()\n",
    "    clustered_trajs[\"cluster_id\"] = clust_id_list\n",
    "    clustered_trajs[\"cluster_color\"] = [colors[i] for i in clust_id_list]\n",
    "    clustered_trajs.to_parquet(f\"{analysis_data_path}/clustering/trajs_simple_connections_factor{factor}.parquet\")\n",
    "else:\n",
    "    clustered_trajs = pd.read_parquet(f\"{analysis_data_path}/clustering/trajs_simple_connections_factor{factor}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "def compute_eccentricity(points):\n",
    "    centroid = np.mean(points, axis=0)\n",
    "    distances = np.linalg.norm(points - centroid, axis=1)\n",
    "    eccentricity = np.std(distances) / np.mean(distances)\n",
    "    return eccentricity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MOTIF ANALYSIS\n",
    "@article {Gal2019\\\n",
    "    author = {Gal, Eyal and Perin, Rodrigo and Markram, Henry and London, Michael and Segev, Idan},\\\n",
    "    title = {Neuron Geometry Underlies a Universal Local Architecture in Neuronal Networks},\\\n",
    "    year = {2019},\\\n",
    "    doi = {10.1101/656058},\\\n",
    "    journal = {bioRxiv}\\\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE MOTIFS TO SEARCH FOR BY HAND\n",
    "frame = 25000\n",
    "\n",
    "X = np.array(rawTrajs.loc[rawTrajs.frame == frame, ['x', 'y']])\n",
    "g, pos = gt.geometric_graph(X, mean_d[frame]*factor)\n",
    "gt.graph_draw(g, pos=pos, output=f'{res_path}/graph/motif/graph_{frame}.png', vertex_size=10, vertex_pen_width=0.5, edge_pen_width=0.5, )\n",
    "\n",
    "n_vertices = 2\n",
    "motifs, counts = gt.motifs(g, n_vertices)\n",
    "for i in range(len(motifs)):\n",
    "    gt.graph_draw(motifs[i], output=f'{res_path}/graph/motif/motif_{frame}_{n_vertices}_{i}.png', vertex_size=10, vertex_pen_width=0.5, edge_pen_width=0.5)\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    sizeList = [3, 4, 5, 5]\n",
    "    for i, motif in enumerate(motifList):\n",
    "        gt.graph_draw(motif, output=f'{res_path}/graph/motif_{i}.png', vertex_size=10)\n",
    "        #motif.save(f'{res_path}/graph/motif_{i}.graphml')\n",
    "else:\n",
    "    sizeList = [3, 4, 5, 5]\n",
    "    motifList = []\n",
    "    for i in range(len(sizeList)):\n",
    "        motifList.append(gt.load_graph(f'{res_path}/graph/motif/selected/motif_{i}.graphml'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_search(mofitList, sizeList, g):\n",
    "    counts = np.zeros(len(mofitList), dtype=int)\n",
    "    for i, mofit in enumerate(mofitList):\n",
    "        _, temp = gt.motifs(g, sizeList[i], motif_list=[mofit])\n",
    "        counts[i] = temp[0]\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_array = np.zeros((len(factorList), len(frames), 4))\n",
    "for i, f in enumerate(factorList):\n",
    "    motif_array[i] = pd.read_parquet(f\"{analysis_data_path}/graph/motif/motif_analysis_factor{f}.parquet\")\n",
    "    fig, (ax, ax1, ax2, ax3) = plt.subplots(4, 1, figsize=(8, 6))\n",
    "    ax.plot(motif_array[i, :, 0])\n",
    "    ax1.plot(motif_array[i, :, 1])\n",
    "    ax2.plot(motif_array[i, :, 2])\n",
    "    ax3.plot(motif_array[i, :, 3])\n",
    "    ax.set(ylim = (0, 400), title = f\"Motif counts - factor {f}\", ylabel=\"triangle\")\n",
    "    ax1.set(ylim = (0, 40), ylabel=\"square\")\n",
    "    ax2.set(ylim = (0, 80), ylabel=\"pentagon\")\n",
    "    ax3.set(xlabel=\"frame\", ylim=(0, 200), ylabel=\"home\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{res_path}/graph/motif/motif_counts_factor{f}.png\", bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 6\n",
    "fig, (ax, ax1, ax2, ax3) = plt.subplots(4, 1, figsize=(8, 6))\n",
    "ax.scatter(motif_array[i, :, 0], motif_array[i, :, 1], s=5, alpha=0.5,  marker='.')\n",
    "ax.set(xlabel=\"triangle\", ylabel=\"square\")\n",
    "ax1.scatter(motif_array[i, :, 1], motif_array[i, :, 2], s=5, alpha=0.5,  marker='.')\n",
    "ax1.set(xlabel=\"square\", ylabel=\"pentagon\")\n",
    "ax2.scatter(motif_array[i, :, 2], motif_array[i, :, 3], s=5, alpha=0.5,  marker='.')\n",
    "ax2.set(xlabel=\"pentagon\", ylabel=\"home\")\n",
    "ax3.scatter(motif_array[i, :, 0], motif_array[i, :, 2], s=5, alpha=0.5,  marker='.')\n",
    "ax3.set(xlabel=\"triangle\", ylabel=\"pentagon\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.plot(factorList, lorentzian_distr(factorList, .3, factorList[int((len(factorList) - 1)/2)]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mix different cutoff distance results with a kernel\n",
    "#kernel = (np.exp(-factorList/factorList[int((len(factorList) - 1)/2)])).reshape(16, 1, 1)\n",
    "kernel = lorentzian_distr(factorList, .3, factorList[int((len(factorList) - 1)/2)]).reshape(16, 1, 1)\n",
    "C = 1/np.sum(kernel)\n",
    "motif_kernel = np.sum(C * kernel * motif_array, axis = 0)\n",
    "\n",
    "fig, (ax, ax1, ax2, ax3) = plt.subplots(4, 1, figsize=(8, 6))\n",
    "ax.plot(motif_kernel[:, 0])\n",
    "ax1.plot(motif_kernel[:, 1])\n",
    "ax2.plot(motif_kernel[:, 2])\n",
    "ax3.plot(motif_kernel[:, 3])\n",
    "ax.set(ylim = (0, 400), title = f\"Motif counts - kernel\", ylabel=\"triangle\")\n",
    "ax1.set(ylim = (0, 40), ylabel=\"square\")\n",
    "ax2.set(ylim = (0, 80), ylabel=\"pentagon\")\n",
    "ax3.set(xlabel=\"frame\", ylim=(0, 200), ylabel=\"home\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 2.2\n",
    "verb_mean = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>n_clusters</th>\n",
       "      <th>n_cycles</th>\n",
       "      <th>degree</th>\n",
       "      <th>degree_centrality</th>\n",
       "      <th>betweenness</th>\n",
       "      <th>clustering</th>\n",
       "      <th>d_cycles</th>\n",
       "      <th>area</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>first_eigv</th>\n",
       "      <th>second_eigv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.723404</td>\n",
       "      <td>0.117755</td>\n",
       "      <td>0.170981</td>\n",
       "      <td>0.390071</td>\n",
       "      <td>3.368421</td>\n",
       "      <td>1441.012281</td>\n",
       "      <td>0.394909</td>\n",
       "      <td>0.057963</td>\n",
       "      <td>0.165463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.680851</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.171512</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>1439.289394</td>\n",
       "      <td>0.394411</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.164874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.680851</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.171512</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>1437.981330</td>\n",
       "      <td>0.394107</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.164874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.680851</td>\n",
       "      <td>0.115095</td>\n",
       "      <td>0.171512</td>\n",
       "      <td>0.354610</td>\n",
       "      <td>3.388889</td>\n",
       "      <td>1434.532224</td>\n",
       "      <td>0.393895</td>\n",
       "      <td>0.057277</td>\n",
       "      <td>0.164874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2.765957</td>\n",
       "      <td>0.119222</td>\n",
       "      <td>0.158992</td>\n",
       "      <td>0.356028</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1434.622534</td>\n",
       "      <td>0.393695</td>\n",
       "      <td>0.088472</td>\n",
       "      <td>0.180004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>29995.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>0.094694</td>\n",
       "      <td>0.087092</td>\n",
       "      <td>0.511524</td>\n",
       "      <td>3.880597</td>\n",
       "      <td>1547.954102</td>\n",
       "      <td>0.342403</td>\n",
       "      <td>0.060189</td>\n",
       "      <td>0.268311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>29996.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>0.094694</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>0.515524</td>\n",
       "      <td>3.880597</td>\n",
       "      <td>1548.189181</td>\n",
       "      <td>0.342952</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>0.269836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>29997.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>0.094694</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>0.515524</td>\n",
       "      <td>3.880597</td>\n",
       "      <td>1548.536712</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>0.269836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>29998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>4.640000</td>\n",
       "      <td>0.094694</td>\n",
       "      <td>0.087585</td>\n",
       "      <td>0.515524</td>\n",
       "      <td>3.880597</td>\n",
       "      <td>1548.881715</td>\n",
       "      <td>0.343697</td>\n",
       "      <td>0.060190</td>\n",
       "      <td>0.269836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>29999.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.680000</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>0.087211</td>\n",
       "      <td>0.516857</td>\n",
       "      <td>3.852941</td>\n",
       "      <td>1548.848277</td>\n",
       "      <td>0.343929</td>\n",
       "      <td>0.061124</td>\n",
       "      <td>0.272426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         frame  n_clusters  n_cycles    degree  degree_centrality  \\\n",
       "0          0.0         2.0      19.0  2.723404           0.117755   \n",
       "1          1.0         2.0      18.0  2.680851           0.115095   \n",
       "2          2.0         2.0      18.0  2.680851           0.115095   \n",
       "3          3.0         2.0      18.0  2.680851           0.115095   \n",
       "4          4.0         2.0      20.0  2.765957           0.119222   \n",
       "...        ...         ...       ...       ...                ...   \n",
       "29995  29995.0         1.0      67.0  4.640000           0.094694   \n",
       "29996  29996.0         1.0      67.0  4.640000           0.094694   \n",
       "29997  29997.0         1.0      67.0  4.640000           0.094694   \n",
       "29998  29998.0         1.0      67.0  4.640000           0.094694   \n",
       "29999  29999.0         1.0      68.0  4.680000           0.095510   \n",
       "\n",
       "       betweenness  clustering  d_cycles         area  eccentricity  \\\n",
       "0         0.170981    0.390071  3.368421  1441.012281      0.394909   \n",
       "1         0.171512    0.354610  3.388889  1439.289394      0.394411   \n",
       "2         0.171512    0.354610  3.388889  1437.981330      0.394107   \n",
       "3         0.171512    0.354610  3.388889  1434.532224      0.393895   \n",
       "4         0.158992    0.356028  3.500000  1434.622534      0.393695   \n",
       "...            ...         ...       ...          ...           ...   \n",
       "29995     0.087092    0.511524  3.880597  1547.954102      0.342403   \n",
       "29996     0.087585    0.515524  3.880597  1548.189181      0.342952   \n",
       "29997     0.087585    0.515524  3.880597  1548.536712      0.343284   \n",
       "29998     0.087585    0.515524  3.880597  1548.881715      0.343697   \n",
       "29999     0.087211    0.516857  3.852941  1548.848277      0.343929   \n",
       "\n",
       "       first_eigv  second_eigv  \n",
       "0        0.057963     0.165463  \n",
       "1        0.057277     0.164874  \n",
       "2        0.057277     0.164874  \n",
       "3        0.057277     0.164874  \n",
       "4        0.088472     0.180004  \n",
       "...           ...          ...  \n",
       "29995    0.060189     0.268311  \n",
       "29996    0.060190     0.269836  \n",
       "29997    0.060190     0.269836  \n",
       "29998    0.060190     0.269836  \n",
       "29999    0.061124     0.272426  \n",
       "\n",
       "[30000 rows x 12 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if verb_mean:\n",
    "    if 0:\n",
    "        res = np.zeros((len(clustered_trajs.frame.unique()), 12))\n",
    "        for frame in tqdm(clustered_trajs.frame.unique()):\n",
    "            df = clustered_trajs.loc[clustered_trajs.frame == frame]\n",
    "            labels = df.cluster_id.values\n",
    "            unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "            degree, degree_centrality, betweenness_centrality, clustering, dim_cycles, eccentricities, area = [], [], [], [], [], [], []\n",
    "            first_eigenvalue, second_eigenvalue = [], []\n",
    "            tot_n_cycles = 0\n",
    "\n",
    "            for j, cluster_id in enumerate(unique_labels[counts>2]):\n",
    "                # create subgrah with positions of the agents in the cluster\n",
    "                df_cluster = df.loc[df.cluster_id == cluster_id]\n",
    "                X = np.array(df_cluster[['x', 'y']])\n",
    "                dicts = {}\n",
    "                for i in range(len(X)):\n",
    "                    dicts[i] = (X[i, 0], X[i, 1])\n",
    "                temp = nx.random_geometric_graph(len(dicts), mean_d[frame]*factor, pos=dicts, dim=2)\n",
    "\n",
    "                # compute area and eccentricity of the subgraph\n",
    "                hull = ConvexHull(X)\n",
    "                area.append(hull.area)\n",
    "                eccentricities.append(compute_eccentricity(X))\n",
    "\n",
    "                # compute degree, degree centrality, betweenness centrality, clustering coefficient, number of cycles,\n",
    "                # dimension of cycles and first and eigenvalue\n",
    "                degree += [val for (_, val) in temp.degree()]\n",
    "                degree_centrality += list(nx.degree_centrality(temp).values())\n",
    "                betweenness_centrality += list(nx.betweenness_centrality(temp).values())\n",
    "                clustering += list(nx.clustering(temp).values())\n",
    "                cycles = nx.cycle_basis(temp)\n",
    "                tot_n_cycles += int(len(cycles))\n",
    "                dim_cycles += [len(cycles[i]) for i in range(int(len(cycles)))]\n",
    "                lap_eigenvalues = nx.laplacian_spectrum(temp)\n",
    "                first_eigenvalue.append(lap_eigenvalues[1])\n",
    "                second_eigenvalue.append(lap_eigenvalues[2])\n",
    "\n",
    "            mean_degree = np.mean(degree)\n",
    "            mean_degree_centrality = np.mean(degree_centrality)\n",
    "            mean_betweenness_centrality = np.mean(betweenness_centrality)\n",
    "            mean_clustering = np.mean(clustering)\n",
    "            if tot_n_cycles == 0:\n",
    "                mean_dim_cycles = 0\n",
    "            else:\n",
    "                mean_dim_cycles = np.mean(dim_cycles)\n",
    "            mean_area = np.mean(area)\n",
    "            mean_eccentricity = np.mean(eccentricities)\n",
    "            mean_first_eigenvalue = np.mean(first_eigenvalue)\n",
    "            mean_second_eigenvalue = np.mean(second_eigenvalue)\n",
    "\n",
    "            res[frame] = np.concatenate(([frame], [len(unique_labels[counts>2])], [tot_n_cycles], [mean_degree], [mean_degree_centrality],\\\n",
    "                                        [mean_betweenness_centrality], [mean_clustering], [mean_dim_cycles], [mean_area], [mean_eccentricity],\\\n",
    "                                        [mean_first_eigenvalue], [mean_second_eigenvalue]))\n",
    "\n",
    "        label = np.array(['frame', 'n_clusters', 'n_cycles', 'degree', 'degree_centrality', 'betweenness', 'clustering',\\\n",
    "                'd_cycles', 'area', 'eccentricity', 'first_eigv', 'second_eigv'])\n",
    "        df_graph = pd.DataFrame(res, columns=label)\n",
    "        display(df_graph)\n",
    "        df_graph.to_parquet(f\"{analysis_data_path}/graph/graph_analysis_mean_factor{factor}.parquet\")\n",
    "    else:\n",
    "        df_graph = pd.read_parquet(f\"{analysis_data_path}/graph/graph_analysis_mean_factor{factor}.parquet\")\n",
    "        label = np.array(df_graph.columns.values, dtype=str)\n",
    "        display(df_graph)\n",
    "else:\n",
    "    hist_bins_degree = np.arange(0, 30, 1)\n",
    "    hist_bins_01 = np.arange(0, 1.01, 0.05)\n",
    "    hist_bins_dim_cycles = np.arange(0, 51, 1)\n",
    "    area_bins = np.arange(100, 2100, 100)\n",
    "\n",
    "    if 0:\n",
    "        res = np.zeros((len(clustered_trajs.frame.unique()), 601))\n",
    "        for frame in tqdm(clustered_trajs.frame.unique()):\n",
    "            df = clustered_trajs.loc[clustered_trajs.frame == frame]\n",
    "            labels = df.cluster_id.values\n",
    "            unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "            degree, degree_centrality, betweenness_centrality, clustering, area, eccentricities, dim_cycles = [], [], [], [], [], [], []\n",
    "            lap_eigenvalues = []\n",
    "            tot_n_cycles = 0\n",
    "\n",
    "            for j, cluster_id in enumerate(unique_labels[counts>2]):\n",
    "                df_cluster = df.loc[df.cluster_id == cluster_id]\n",
    "                X = np.array(df_cluster[['x', 'y']])\n",
    "                # create dictionary with positions of the agents\n",
    "                dicts = {}\n",
    "                for i in range(len(X)):\n",
    "                    dicts[i] = (X[i, 0], X[i, 1])\n",
    "                temp = nx.random_geometric_graph(len(dicts), mean_d[frame]*factor, pos=dicts, dim=2)\n",
    "\n",
    "                # compute area and eccentricity of the subgraph\n",
    "                hull = ConvexHull(X)\n",
    "                area.append(hull.area)\n",
    "                eccentricities.append(compute_eccentricity(X))\n",
    "                \n",
    "                degree += [val for (_, val) in temp.degree()]\n",
    "                degree_centrality += list(nx.degree_centrality(temp).values())\n",
    "                betweenness_centrality += list(nx.betweenness_centrality(temp).values())\n",
    "                clustering += list(nx.clustering(temp).values())\n",
    "\n",
    "                cycles = nx.cycle_basis(temp)\n",
    "                tot_n_cycles += int(len(cycles))\n",
    "                dim_cycles += [len(cycles[i]) for i in range(int(len(cycles)))]\n",
    "                lap_eigenvalues += list(nx.laplacian_spectrum(temp))\n",
    "        \n",
    "            degree_count,             _ = np.histogram(degree, bins=hist_bins_degree)\n",
    "            centrality_count,         _ = np.histogram(degree_centrality, bins=hist_bins_01) \n",
    "            betweenness_count,        _ = np.histogram(betweenness_centrality, bins=hist_bins_01)\n",
    "            clustering_count,         _ = np.histogram(clustering, bins=hist_bins_01)\n",
    "            dim_cycles_count,         _ = np.histogram(dim_cycles, bins=hist_bins_dim_cycles)\n",
    "            area,                     _ = np.histogram(area, bins=area_bins)\n",
    "            eccentricity,             _ = np.histogram(eccentricities, bins=hist_bins_01)\n",
    "            laplacian_spectrum_count, _ = np.histogram(lap_eigenvalues, bins=hist_bins_01)\n",
    "            \n",
    "            res[frame] =  np.concatenate(([frame], [len(unique_labels[counts>2])], [tot_n_cycles], degree_count, centrality_count,\\\n",
    "                                        betweenness_count, clustering_count, dim_cycles_count, area, eccentricity,\\\n",
    "                                        laplacian_spectrum_count))\n",
    "        label = np.array(['frame'] + ['nClusters'] + ['nCycles'] + [f'degree_{i}' for i in range(len(hist_bins_degree[1:]))] + \\\n",
    "                [f'centrality_{i}' for i in range(len(hist_bins_01[1:]))] + [f'betweenness_{i}' for i in range(len(hist_bins_01[1:]))] + \\\n",
    "                [f'clustering_{i}' for i in range(len(hist_bins_01[1:]))] + [f'dim_cycles_{i}' for i in range(len(hist_bins_dim_cycles[1:]))] +\\\n",
    "                [f'area_{i}' for i in range(len(area_bins[1:]))] + [f'eccentricity_{i}' for i in range(len(hist_bins_01[1:]))] +\\\n",
    "                [f'laplacian_{i}'for i in range(len(hist_bins_01[1:]))])\n",
    "        df_graph = pd.DataFrame(res, columns=label)\n",
    "        df_graph.to_parquet(f'{analysis_data_path}/graph/graph_analysis_factor{factor}.parquet')\n",
    "    else:\n",
    "        df_graph = pd.read_parquet(f'{analysis_data_path}/graph/graph_analysis_factor{factor}.parquet')\n",
    "        label = np.array(df_graph.columns.values, dtype=str)\n",
    "        display(df_graph)\n",
    "\n",
    "    # how to index the array\n",
    "    id_nClusters = np.argwhere(np.char.startswith(label, 'nClusters')==True)[0][0]\n",
    "    id_nCycles = np.argwhere(np.char.startswith(label, 'nCycles')==True)[0][0]\n",
    "    id_degree = np.argwhere(np.char.startswith(label, 'degree')==True)[0][0], np.argwhere(np.char.startswith(label, 'degree')==True)[-1][0]\n",
    "    id_centrality = np.argwhere(np.char.startswith(label, 'centrality')==True)[0][0], np.argwhere(np.char.startswith(label, 'centrality')==True)[-1][0]\n",
    "    id_betweenness = np.argwhere(np.char.startswith(label, 'betweenness')==True)[0][0], np.argwhere(np.char.startswith(label, 'betweenness')==True)[-1][0]\n",
    "    id_clustering = np.argwhere(np.char.startswith(label, 'clustering')==True)[0][0], np.argwhere(np.char.startswith(label, 'clustering')==True)[-1][0]\n",
    "    id_dim_cycles = np.argwhere(np.char.startswith(label, 'dim_cycles')==True)[0][0], np.argwhere(np.char.startswith(label, 'dim_cycles')==True)[-1][0]\n",
    "    id_area = np.argwhere(np.char.startswith(label, 'area')==True)[0][0], np.argwhere(np.char.startswith(label, 'area')==True)[-1][0]\n",
    "    id_eccentricity = np.argwhere(np.char.startswith(label, 'eccentricity')==True)[0][0], np.argwhere(np.char.startswith(label, 'eccentricity')==True)[-1][0]\n",
    "    id_laplacian = np.argwhere(np.char.startswith(label, 'laplacian')==True)[0][0], np.argwhere(np.char.startswith(label, 'laplacian')==True)[-1][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dimensionality reduction procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist or mean ?\n",
    "x = df_graph.iloc[:, 1:].values # 1: to skip the frame column\n",
    "x = StandardScaler().fit_transform(x) # normalizing the features\n",
    "x = x/np.std(x)\n",
    "print(x.shape, np.mean(x), np.std(x))\n",
    "normalized_df = pd.DataFrame(x, columns=label[1:])\n",
    "\n",
    "# explained variance ratio with PCA\n",
    "pca = PCA(n_components=label.shape[0]-1)\n",
    "p_components = pca.fit_transform(x)\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "ax.plot(np.arange(1, p_components.shape[1]+1, 1), pca.explained_variance_ratio_.cumsum())\n",
    "if verb_mean: ax.set(xlabel='n components', ylabel = 'cumulative explained variance ratio', title = 'PCA scree plot - mean features')\n",
    "else: ax.set(xlabel='n components', ylabel = 'cumulative explained variance ratio', title = 'PCA scree plot - histogram features')\n",
    "ax.grid()\n",
    "if verb_mean:\n",
    "    plt.savefig(f'{res_path}/graph/scree_plot_mean.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f'{res_path}/graph/scree_plot_hist.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "p_components = pca.fit_transform(x)\n",
    "expl_variance_ratio = np.round(pca.explained_variance_ratio_,2)\n",
    "print('Explained variation per principal component: {}'.format(expl_variance_ratio))\n",
    "principal_df = pd.DataFrame(data = p_components, columns = ['pc1', 'pc2', 'pc3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize = (12, 4))\n",
    "scatt = axs[0].scatter(principal_df.pc1, principal_df.pc2, c=df_graph['frame'], cmap='viridis', s=10)\n",
    "axs[0].set(xlabel = f'PC1 ({expl_variance_ratio[0]})', ylabel = f'PC2 ({expl_variance_ratio[1]})')\n",
    "axs[1].scatter(principal_df.pc2, principal_df.pc3, c=df_graph['frame'], cmap='viridis', s=10)\n",
    "axs[1].set(xlabel = f'PC2 ({expl_variance_ratio[1]})', ylabel = f'PC3 ({expl_variance_ratio[2]})')\n",
    "axs[2].scatter(principal_df.pc1, principal_df.pc3, c=df_graph['frame'], cmap='viridis', s=10)\n",
    "axs[2].set(xlabel = f'PC1 ({expl_variance_ratio[0]})', ylabel = f'PC3 ({expl_variance_ratio[2]})')\n",
    "if verb_mean: plt.suptitle('PCA - mean features')\n",
    "else: plt.suptitle('PCA - histogram features')\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "axs[2].grid()\n",
    "#fig.colorbar(scatt, ax=axs, label='frame', orientation='horizontal', shrink=0.5)\n",
    "plt.tight_layout()\n",
    "if verb_mean:\n",
    "    plt.savefig(f'{res_path}/graph/mean_metrics/pca_mean.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f'{res_path}/graph/hist_metrics/pca_hist.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for mean only\n",
    "if verb_mean:\n",
    "    loadings = pca.components_\n",
    "    fig, (ax, ax1, ax2) = plt.subplots(3, 1, figsize = (10, 6), sharex=True, sharey=True)\n",
    "    ax.plot(loadings[0], 'r', label = 'PC1')\n",
    "    ax1.plot(loadings[1], 'b', label = 'PC2')\n",
    "    ax2.plot(loadings[2], 'y', label = 'PC3')\n",
    "    ax.set(ylabel = 'PC1', title = 'PCA components - mean', ylim=(-1,1))\n",
    "    ax1.set(ylabel = 'PC2', ylim=(-1,1))\n",
    "    ax2.set(ylabel = 'PC3', xlabel = 'features', ylim=(-1,1), xticks = np.arange(loadings.shape[1]))\n",
    "    ax2.set_xticklabels(df_graph.columns[1:], rotation = 45)\n",
    "    ax.grid(linewidth = 0.5)\n",
    "    ax1.grid(linewidth = 0.5)\n",
    "    ax2.grid(linewidth = 0.5)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{res_path}/graph/mean_metrics/pca_components_mean.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# for hist only\n",
    "else:\n",
    "    loadings = pca.components_\n",
    "    regions = [(id_degree[0], id_degree[1]+1, 'blue'), (id_centrality[0], id_centrality[1]+1, 'red'),\\\n",
    "            (id_betweenness[0], id_betweenness[1]+1, 'green'), (id_clustering[0], id_clustering[1]+1, 'orange'),\\\n",
    "            (id_dim_cycles[0], id_dim_cycles[1]+1, 'purple'), (id_area[0], id_area[1]+1, 'brown'),\\\n",
    "            (id_eccentricity[0], id_eccentricity[1]+1, 'magenta'), (id_laplacian[0], id_laplacian[1]+1, 'yellow')]\n",
    "\n",
    "    fig, (ax, ax1, ax2) = plt.subplots(3, 1, figsize = (10, 6), sharex=True, sharey=True)\n",
    "    ax.plot(loadings[0], 'r')\n",
    "    ax1.plot(loadings[1], 'g')\n",
    "    ax2.plot(loadings[2], 'b')\n",
    "    ax.set(ylabel  = 'loadings pc1', title = 'loadings - hist')\n",
    "    ax1.set(ylabel = 'loadings pc2')\n",
    "    ax2.set(ylabel = 'loadings pc3', xlabel = 'features')\n",
    "    ax.axvspan(0, 2, facecolor = 'brown', alpha=0.2, label = \"clusters & cycles\")\n",
    "    ax1.axvspan(0, 2, facecolor = 'brown', alpha=0.2)\n",
    "    ax2.axvspan(0, 2, facecolor = 'brown', alpha=0.2)\n",
    "    for region in regions:\n",
    "        ax.axvspan( region[0]-1, region[1]-1, facecolor = region[2], alpha=0.2, label = label[region[0]].split('_')[0])\n",
    "        ax1.axvspan(region[0]-1, region[1]-1, facecolor = region[2], alpha=0.2)\n",
    "        ax2.axvspan(region[0]-1, region[1]-1, facecolor = region[2], alpha=0.2)\n",
    "    ax.set_ylim(-1,1)\n",
    "    ax1.set_ylim(-1,1)\n",
    "    ax2.set_ylim(-1,1)\n",
    "    ax.grid(linewidth = 0.5)\n",
    "    ax1.grid(linewidth = 0.5)\n",
    "    ax2.grid(linewidth = 0.5)\n",
    "    fig.legend(loc=7)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    plt.savefig(f'{res_path}/graph/hist_metrics/pca_components_hist.png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "scatterplot = ax.scatter(principal_df['pc1'], principal_df['pc2'], principal_df['pc3'], c = frames, cmap = 'viridis', s = 1)\n",
    "ax.grid()\n",
    "fig.colorbar(scatterplot, ax=ax, shrink=0.6, label = 'frame', orientation = 'horizontal')\n",
    "ax.set(xlabel = f'PC1 ({expl_variance_ratio[0]})', ylabel = f'PC2 ({expl_variance_ratio[1]})', zlabel = f'PC3 ({expl_variance_ratio[2]})')\n",
    "if 0:\n",
    "    # save to gif\n",
    "    import imageio\n",
    "    images = []\n",
    "    for n in range(0, 250):\n",
    "        if n >= 5:\n",
    "            ax.azim = ax.azim+1.1\n",
    "        fig.canvas.draw()\n",
    "        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        images.append(image.reshape(600, 600, 3)) ## \n",
    "    if verb_mean: \n",
    "        imageio.mimsave(f'{res_path}/graph/pca_mean.gif', images)\n",
    "    else:\n",
    "        imageio.mimsave(f'{res_path}/graph/pca_hist_V2.gif', images)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 200) 6.497392253853226e-15 1.0000000000000004\n",
      "Explained variation per principal component: [0.15 0.06 0.05]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'id_degree' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/matteoscandola/MasterThesis/analysis/dev.ipynb Cell 35\u001b[0m in \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m# for hist only\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     loadings \u001b[39m=\u001b[39m pca\u001b[39m.\u001b[39mcomponents_\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     regions \u001b[39m=\u001b[39m [(id_degree[\u001b[39m0\u001b[39m], id_degree[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mblue\u001b[39m\u001b[39m'\u001b[39m), (id_centrality[\u001b[39m0\u001b[39m], id_centrality[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mred\u001b[39m\u001b[39m'\u001b[39m),\\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m             (id_betweenness[\u001b[39m0\u001b[39m], id_betweenness[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mgreen\u001b[39m\u001b[39m'\u001b[39m), (id_clustering[\u001b[39m0\u001b[39m], id_clustering[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39morange\u001b[39m\u001b[39m'\u001b[39m),\\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m             (id_dim_cycles[\u001b[39m0\u001b[39m], id_dim_cycles[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpurple\u001b[39m\u001b[39m'\u001b[39m), (id_area[\u001b[39m0\u001b[39m], id_area[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mbrown\u001b[39m\u001b[39m'\u001b[39m),\\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m             (id_eccentricity[\u001b[39m0\u001b[39m], id_eccentricity[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmagenta\u001b[39m\u001b[39m'\u001b[39m), (id_laplacian[\u001b[39m0\u001b[39m], id_laplacian[\u001b[39m1\u001b[39m]\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39myellow\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     fig, (ax, ax1, ax2) \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, figsize \u001b[39m=\u001b[39m (\u001b[39m10\u001b[39m, \u001b[39m6\u001b[39m), sharex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, sharey\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/matteoscandola/MasterThesis/analysis/dev.ipynb#Y204sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     ax\u001b[39m.\u001b[39mplot(loadings[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id_degree' is not defined"
     ]
    }
   ],
   "source": [
    "verb_mean = False\n",
    "if verb_mean:\n",
    "    datasetArray = np.zeros((len(factorList), 30000, 12))\n",
    "    for i,f in enumerate(factorList):\n",
    "        datasetArray[i] = pd.read_parquet(f\"{analysis_data_path}/graph/graph_analysis_mean_factor{f}.parquet\")\n",
    "else: \n",
    "    datasetArray = np.zeros((len(factorList), 30000, 201))\n",
    "    for i,f in enumerate(factorList):\n",
    "        datasetArray[i] = pd.read_parquet(f\"{analysis_data_path}/graph/graph_analysis_factor{f}.parquet\")\n",
    "\n",
    "# mix different cutoff distance results with a kernel\n",
    "kernel = (np.exp(-factorList/factorList[int((len(factorList) - 1)/2)])).reshape(16, 1, 1)\n",
    "#kernel = lorentzian_distr(factorList, .3, factorList[int((len(factorList) - 1)/2)]).reshape(16, 1, 1)\n",
    "C = 1/np.sum(kernel)\n",
    "datasetKernel = np.sum(C * kernel * datasetArray, axis = 0)\n",
    "x = datasetKernel[:, 1:] # 1: to skip the frame column\n",
    "x = StandardScaler().fit_transform(x) # normalizing the features\n",
    "x = x/np.std(x)\n",
    "print(x.shape, np.mean(x), np.std(x))\n",
    "\n",
    "# explained variance ratio with PCA\n",
    "pca = PCA(n_components=x.shape[1]-1)\n",
    "p_components = pca.fit_transform(x)\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "ax.plot(np.arange(1, p_components.shape[1]+1, 1), pca.explained_variance_ratio_.cumsum())\n",
    "if verb_mean: ax.set(xlabel='n components', ylabel = 'cumulative explained variance ratio', title = 'PCA scree plot - mean features')\n",
    "else: ax.set(xlabel='n components', ylabel = 'cumulative explained variance ratio', title = 'PCA scree plot - histogram features')\n",
    "ax.grid()\n",
    "if verb_mean:\n",
    "    plt.savefig(f'{res_path}/graph/scree_plot_mean_kernel2.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f'{res_path}/graph/scree_plot_hist_kernel2.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "p_components = pca.fit_transform(x)\n",
    "expl_variance_ratio = np.round(pca.explained_variance_ratio_,2)\n",
    "print('Explained variation per principal component: {}'.format(expl_variance_ratio))\n",
    "principal_df = pd.DataFrame(data = p_components, columns = ['pc1', 'pc2', 'pc3'])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize = (12, 4))\n",
    "scatt = axs[0].scatter(principal_df.pc1, principal_df.pc2, c=frames, cmap='viridis', s=10)\n",
    "axs[0].set(xlabel = f'PC1 ({expl_variance_ratio[0]})', ylabel = f'PC2 ({expl_variance_ratio[1]})')\n",
    "axs[1].scatter(principal_df.pc2, principal_df.pc3, c=frames, cmap='viridis', s=10)\n",
    "axs[1].set(xlabel = f'PC2 ({expl_variance_ratio[1]})', ylabel = f'PC3 ({expl_variance_ratio[2]})')\n",
    "axs[2].scatter(principal_df.pc1, principal_df.pc3, c=frames, cmap='viridis', s=10)\n",
    "axs[2].set(xlabel = f'PC1 ({expl_variance_ratio[0]})', ylabel = f'PC3 ({expl_variance_ratio[2]})')\n",
    "if verb_mean: plt.suptitle('PCA - mean features')\n",
    "else: plt.suptitle('PCA - histogram features')\n",
    "axs[0].grid()\n",
    "axs[1].grid()\n",
    "axs[2].grid()\n",
    "#fig.colorbar(scatt, ax=axs, label='frame', orientation='horizontal', shrink=0.5)\n",
    "plt.tight_layout()\n",
    "if verb_mean:\n",
    "    plt.savefig(f'{res_path}/graph/pca_mean_kernel2.png', bbox_inches='tight')\n",
    "else:\n",
    "    plt.savefig(f'{res_path}/graph/pca_hist_kernel2.png', bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "scatterplot = ax.scatter(principal_df['pc1'], principal_df['pc2'], principal_df['pc3'], c = frames, cmap = 'viridis', s = 1)\n",
    "ax.grid()\n",
    "fig.colorbar(scatterplot, ax=ax, shrink=0.6, label = 'frame', orientation = 'horizontal')\n",
    "ax.set(xlabel = f'PC1 ({expl_variance_ratio[0]})', ylabel = f'PC2 ({expl_variance_ratio[1]})', zlabel = f'PC3 ({expl_variance_ratio[2]})')\n",
    "if 1:\n",
    "    # save to gif\n",
    "    import imageio\n",
    "    images = []\n",
    "    for n in range(0, 250):\n",
    "        if n >= 5:\n",
    "            ax.azim = ax.azim+1.1\n",
    "        fig.canvas.draw()\n",
    "        image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "        images.append(image.reshape(600, 600, 3)) ## \n",
    "    if verb_mean: \n",
    "        imageio.mimsave(f'{res_path}/graph/pca_mean_kernel2.gif', images)\n",
    "    else:\n",
    "        imageio.mimsave(f'{res_path}/graph/pca_hist_kernel2.gif', images)\n",
    "plt.close()\n",
    "\n",
    "# for mean only\n",
    "if verb_mean:\n",
    "    loadings = pca.components_\n",
    "    fig, (ax, ax1, ax2) = plt.subplots(3, 1, figsize = (10, 6), sharex=True, sharey=True)\n",
    "    ax.plot(loadings[0], 'r', label = 'PC1')\n",
    "    ax1.plot(loadings[1], 'b', label = 'PC2')\n",
    "    ax2.plot(loadings[2], 'y', label = 'PC3')\n",
    "    ax.set(ylabel = 'PC1', title = 'PCA components - mean', ylim=(-1,1))\n",
    "    ax1.set(ylabel = 'PC2', ylim=(-1,1))\n",
    "    ax2.set(ylabel = 'PC3', xlabel = 'features', ylim=(-1,1), xticks = np.arange(loadings.shape[1]))\n",
    "    ax2.set_xticklabels(df_graph.columns[1:], rotation = 45)\n",
    "    ax.grid(linewidth = 0.5)\n",
    "    ax1.grid(linewidth = 0.5)\n",
    "    ax2.grid(linewidth = 0.5)\n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{res_path}/graph/mean_metrics/pca_components_mean_kernel2.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# for hist only\n",
    "else:\n",
    "    loadings = pca.components_\n",
    "    regions = [(id_degree[0], id_degree[1]+1, 'blue'), (id_centrality[0], id_centrality[1]+1, 'red'),\\\n",
    "            (id_betweenness[0], id_betweenness[1]+1, 'green'), (id_clustering[0], id_clustering[1]+1, 'orange'),\\\n",
    "            (id_dim_cycles[0], id_dim_cycles[1]+1, 'purple'), (id_area[0], id_area[1]+1, 'brown'),\\\n",
    "            (id_eccentricity[0], id_eccentricity[1]+1, 'magenta'), (id_laplacian[0], id_laplacian[1]+1, 'yellow')]\n",
    "\n",
    "    fig, (ax, ax1, ax2) = plt.subplots(3, 1, figsize = (10, 6), sharex=True, sharey=True)\n",
    "    ax.plot(loadings[0], 'r')\n",
    "    ax1.plot(loadings[1], 'g')\n",
    "    ax2.plot(loadings[2], 'b')\n",
    "    ax.set(ylabel  = 'loadings pc1', title = 'loadings - hist')\n",
    "    ax1.set(ylabel = 'loadings pc2')\n",
    "    ax2.set(ylabel = 'loadings pc3', xlabel = 'features')\n",
    "    ax.axvspan(0, 2, facecolor = 'brown', alpha=0.2, label = \"clusters & cycles\")\n",
    "    ax1.axvspan(0, 2, facecolor = 'brown', alpha=0.2)\n",
    "    ax2.axvspan(0, 2, facecolor = 'brown', alpha=0.2)\n",
    "    for region in regions:\n",
    "        ax.axvspan( region[0]-1, region[1]-1, facecolor = region[2], alpha=0.2, label = label[region[0]].split('_')[0])\n",
    "        ax1.axvspan(region[0]-1, region[1]-1, facecolor = region[2], alpha=0.2)\n",
    "        ax2.axvspan(region[0]-1, region[1]-1, facecolor = region[2], alpha=0.2)\n",
    "    ax.set_ylim(-1,1)\n",
    "    ax1.set_ylim(-1,1)\n",
    "    ax2.set_ylim(-1,1)\n",
    "    ax.grid(linewidth = 0.5)\n",
    "    ax1.grid(linewidth = 0.5)\n",
    "    ax2.grid(linewidth = 0.5)\n",
    "    fig.legend(loc=7)\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    plt.savefig(f'{res_path}/graph/hist_metrics/pca_components_hist_kernel2.png', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coordination number analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@joblib.delayed\n",
    "def rdf_frame(frame, COORDS, rList, dr, rho):\n",
    "    coords = COORDS[frame*nDrops:(frame+1)*nDrops,:]\n",
    "    kd = KDTree(coords)\n",
    "    avg_n = np.zeros(len(rList))\n",
    "    for i, r in enumerate(rList):\n",
    "        a = kd.query_ball_point(coords, r + 20)\n",
    "        b = kd.query_ball_point(coords, r)\n",
    "        n1 = 0\n",
    "        for j in a:\n",
    "            n1 += len(j) - 1\n",
    "        n2 = 0\n",
    "        for j in b:\n",
    "            n2 += len(j) - 1\n",
    "        avg_n[i] = n1/len(a) - n2/len(b)\n",
    "    rdf = avg_n/(np.pi*(dr**2 + 2*rList*dr)*rho)\n",
    "    return rdf\n",
    "\n",
    "\n",
    "def get_rdf(run_analysis_verb, nFrames, trajs, rList, dr, rho):\n",
    "    print(trajs)\n",
    "    if trajs == \"raw\":\n",
    "        trajectories = rawTrajs\n",
    "    elif trajs == \"smooth\":\n",
    "        trajectories = smoothTrajs\n",
    "    else:\n",
    "        raise ValueError(\"trajs must be 'raw' or 'smooth'\")\n",
    "     \n",
    "    if run_analysis_verb:\n",
    "        COORDS = np.array(trajectories.loc[:, [\"x\",\"y\"]])\n",
    "        parallel = joblib.Parallel(n_jobs = -2)\n",
    "        rdf = parallel(\n",
    "            rdf_frame(frame, COORDS, rList, dr, rho)\n",
    "            for frame in tqdm(range(nFrames))\n",
    "        )\n",
    "        rdf = np.array(rdf)\n",
    "        rdf_df = pd.DataFrame(rdf)\n",
    "        # string columns for parquet filetype\n",
    "        rdf_df.columns = [f\"{r}\" for r in rList]\n",
    "        rdf_df.to_parquet(f\"./{analysis_data_path}/rdf/rdf_{trajs}.parquet\")\n",
    "\n",
    "    elif not run_analysis_verb :\n",
    "        try:\n",
    "            rdf = np.array(pd.read_parquet(f\"./{analysis_data_path}/rdf/rdf_{trajs}.parquet\"))\n",
    "        except: \n",
    "            raise ValueError(\"rdf data not found. Run analysis verbosely first.\")\n",
    "    else: \n",
    "        raise ValueError(\"run_analysis_verb must be True or False\")\n",
    "    return rdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = 5\n",
    "rDisk = 822/2\n",
    "rList = np.arange(0, 2*rDisk, 1)\n",
    "rho = nDrops/(np.pi*rDisk**2) # nDrops - 1 !???\n",
    "\n",
    "print(\"RDF - Raw Trajectories\")\n",
    "rdf_raw = get_rdf(True, 200, \"raw\", rList, dr, rho)\n",
    "#print(\"RDF - Smooth Trajectories\")\n",
    "#rdf_smooth = get_rdf(run_analysis_verb, nFrames, \"smooth\", rList, dr, rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10,4))\n",
    "ax.plot(rdf_raw[0])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK BALLISTIC REGIME BETWEEN 0-1 S - FOR SMOOTH TRAJECTORIES\n",
    "\n",
    "note that ballistic motion msd has pw exponent 2 !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 10000, 0.1)\n",
    "y = 10*x + 2*x**2\n",
    "# compute msd\n",
    "max_lagtime = 1000\n",
    "msd = np.zeros(max_lagtime-1)\n",
    "for lagtime in range(1, max_lagtime):\n",
    "    msd[lagtime-1] = np.mean((y[lagtime:] - y[:-lagtime])**2 + (x[lagtime:] - x[:-lagtime])**2)\n",
    "# fit msd with power law\n",
    "def power_law(x, a, b):\n",
    "    return a*x**b\n",
    "popt, pcov = curve_fit(power_law, np.arange(1, max_lagtime), msd, p0 = (1, 1))\n",
    "print(f\"fit parameters: {popt}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (6, 6))\n",
    "ax.plot(np.arange(1, max_lagtime), msd)\n",
    "ax.set(xlabel=\"lagtime\", ylabel=\"msd\", title = \"MSD of a parabola\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pxDimension = 1 # has to be fixed \n",
    "fps = 10 # fps of the video\n",
    "maxLagtime = 1000 # maximum lagtime to be considered\n",
    "#x = np.array(imsd[1:].index)\n",
    "x = np.arange(1., 100.1, .1)\n",
    "imsd, fit, pw_exp = get_imsd(rawTrajs, pxDimension, fps, maxLagtime, nDrops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1) = plt.subplots(2, 1, figsize = (8, 4))\n",
    "ax.plot(imsd.index, imsd, '.',  markersize = 1)\n",
    "ax.plot(imsd[:1].index, fit_ball.T, linewidth = 1)\n",
    "ax.plot(imsd[1:].index, fit.T, linewidth = 1)\n",
    "ax.set(xlabel=\"lag time [s]\", ylabel=\"MSD [px$^2$]\", xscale = \"log\", yscale = \"log\", xlim=(0.01, 10))\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "\n",
    "ax1.errorbar(np.arange(0, nDrops, 1), pw_exp_ball[:, 0, 1], yerr = pw_exp_ball[:, 1, 0], fmt = '.', markersize = 5, label = \"ballistic\")\n",
    "ax1.errorbar(np.arange(0, nDrops, 1), pw_exp[:, 0, 1], yerr = pw_exp_ball[:, 1, 0], fmt = '.', markersize = 5, label = \"diffusive\")\n",
    "ax1.set(xlabel=\"Droplet ID\", ylabel=\"Pw exponent\")\n",
    "ax1.grid(True, linestyle='-', color = '0.75')\n",
    "ax1.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSD_b, MSD_r, diffusive_results, ballistic_results = get_emsd(imsd, x, red_particle_idx, nDrops)\n",
    "fit_b_ball, pw_exp_b_ball = ballistic_results[\"fit_b\"], ballistic_results[\"pw_exp_b\"]\n",
    "fit_b_diff, pw_exp_b_diff = diffusive_results[\"fit_b\"], diffusive_results[\"pw_exp_b\"]\n",
    "fit_r_ball, pw_exp_r_ball = ballistic_results[\"fit_r\"], ballistic_results[\"pw_exp_r\"]\n",
    "fit_r_diff, pw_exp_r_diff = diffusive_results[\"fit_r\"], diffusive_results[\"pw_exp_r\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ball = [round(pw_exp_b_ball[0, 1], 3), round(pw_exp_b_ball[1, 1], 3)]\n",
    "b_ball = [round(pw_exp_r_ball[0, 1], 3), round(pw_exp_r_ball[1, 1], 3)]\n",
    "print(f\"Smooth trajs - Ballistic - Blue Particles: {a_ball[0]} ± {a_ball[1]}, Red Particle: {b_ball[0]} ± {b_ball[1]}\")\n",
    "\n",
    "a_diff = [round(pw_exp_b_diff[0, 1], 3), round(pw_exp_b_diff[1, 1], 3)]\n",
    "b_diff = [round(pw_exp_r_diff[0, 1], 3), round(pw_exp_r_diff[1, 1], 3)]\n",
    "print(f\"Smooth trajs - Diffusive - Blue Particles: {a_diff[0]} ± {a_diff[1]}, Red Particle: {b_diff[0]} ± {b_diff[1]}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "ax.plot(imsd.index, MSD_b[0], 'b-', label = \"Blue particles\") \n",
    "ax.plot(imsd[1:].index, fit_b_diff, 'b--', label = f\"Diffusive Region: {a_diff[0]} ± {a_diff[1]}\")\n",
    "ax.plot(imsd[:1].index, fit_b_ball, 'b:', label = f\"Ballistic Region: {a_ball[0]} ± {a_ball[1]}\")\n",
    "ax.fill_between(imsd.index, MSD_b[0] - MSD_b[1], MSD_b[0] + MSD_b[1], alpha=0.5, edgecolor='#00FFFF', facecolor='#F0FFFF')\n",
    "ax.plot(imsd.index, MSD_r, 'r-', label = \"Red particle\")\n",
    "ax.plot(imsd[1:].index, fit_r_diff, 'r--', label = f\"Diffusive Region: {b_diff[0]} ± {b_diff[1]}\")\n",
    "ax.plot(imsd[:1].index, fit_r_ball, 'r:', label = f\"Ballistic Region: {b_ball[0]} ± {b_ball[1]}\")\n",
    "ax.set(xscale = 'log', yscale = 'log', ylabel = r'$\\langle \\Delta r^2 \\rangle$ [$px^2$]',   \n",
    "        xlabel = 'lag time $t$ [s]', title = \"EMSD - Smooth Trajectories\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "plt.savefig(f\"./{res_path}/mean_squared_displacement/ballistic_region/EMSD_smoothTrajs.png\", bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMENSION OF DROPLETS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_wind = rawTrajs.groupby(\"frame\").mean().r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "ax.plot(mean_wind.index/10, mean_wind)\n",
    "ax.set(xlabel = \"Time [s]\", ylabel = \"r [px]\", title = \"Droplets mean radius\")\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "if save_verb: plt.savefig(res_path + \"/dimension_analysis/mean_radius.png\", bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "temp = rawTrajs[rawTrajs.particle == red_particle_idx]\n",
    "temp_d = temp.r#.rolling(window = 100).mean()\n",
    "ax.plot(temp.frame/10, temp_d, 'r-', zorder = 10)\n",
    "for i in range(nDrops):\n",
    "    if i != red_particle_idx:\n",
    "        temp = rawTrajs[rawTrajs.particle == i]\n",
    "        temp_d = temp.r#.rolling(window = 100).mean()\n",
    "        ax.plot(temp.frame.values.astype(int)/10, temp_d, 'b-', alpha = 0.4, linewidth = 0.5, zorder = 0)\n",
    "ax.set(xlabel = \"Time [s]\", ylabel = \"r [px]\", title = \"Droplet radius moving averaged over 100 frames\")\n",
    "ax.grid()\n",
    "ax.legend([\"Red droplet\", \"Blue droplets\"])\n",
    "if save_verb: plt.savefig(res_path + \"/dimension_analysis/radius_mov_avg_100.png\", bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## windowed radius vs thingz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std droplet diameter per frame\n",
    "# windowed ?\n",
    "nFrames =  max(rawTrajs.frame) + 1\n",
    "print(nFrames)\n",
    "# WINDOWED ANALYSIS PARAMETERS\n",
    "window = 3200 # 320 s\n",
    "stride = 100 # 10 s\n",
    "print(f\"window of {window/10} s, stride of {stride/10} s\")\n",
    "startFrames = np.arange(0, nFrames-window, stride, dtype=int)\n",
    "endFrames = startFrames + window\n",
    "nSteps = len(startFrames)\n",
    "print(f\"number of steps: {nSteps}\")\n",
    "\n",
    "mean_r_wind = np.zeros(nSteps)\n",
    "for i, start in enumerate(startFrames):\n",
    "    temp = rawTrajs.loc[rawTrajs.frame.between(start, start+window)]\n",
    "    mean_r_wind[i] = np.nanmean(temp.r.values)\n",
    "\n",
    "d_wind = np.zeros((nSteps, nDrops))\n",
    "d_wind_std = np.zeros((nSteps, nDrops))\n",
    "for i, start in enumerate(startFrames):\n",
    "    temp = rawTrajs.loc[rawTrajs.frame.between(start, start+window)]\n",
    "    for j in range(nDrops):\n",
    "        temp_j = temp.loc[temp.particle == j].r.values\n",
    "        d_wind[i, j] = np.nanmean(temp_j)\n",
    "        d_wind_std[i, j] = np.nanstd(temp_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "ax.plot(startFrames/10, mean_r_wind, 'k--', linewidth = 2, zorder=20)\n",
    "ax.plot(startFrames/10, d_wind[:, red_particle_idx], 'r-', linewidth = 2, zorder=10)\n",
    "for i in range(nDrops):\n",
    "    if i != red_particle_idx:\n",
    "        ax.plot(startFrames/10, d_wind[:, i], 'b-', zorder=0, alpha = 0.5)\n",
    "ax.set(xlabel = \"Window time [s]\", ylabel = \"r [px]\", title = \"Droplet radius by window time\")\n",
    "ax.grid()\n",
    "ax.legend([\"Mean\", \"Red droplet\", \"Blue droplets\"])\n",
    "if save_verb: plt.savefig(res_path + \"/dimension_analysis/radius_wind.png\", bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vs power law exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_verb = False\n",
    "run_windowed_analysis = True\n",
    "plot_verb = False\n",
    "animated_plot_verb = False\n",
    "print(f\"MSD Analysis: show_verb = {show_verb}, run_windowed_analysis = {run_windowed_analysis}, animated_plot_verb = {animated_plot_verb}\")\n",
    "\n",
    "%run ./analysis_modules/msd.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_pw_law = pw_exp_wind[:, :, 0, 1]\n",
    "ensemble_pw_lab_b = fit_dict[\"pw_exp_wind_b\"][:, 0, 1]\n",
    "ensemble_pw_lab_r = fit_dict[\"pw_exp_wind_r\"][:, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 5))\n",
    "ax.plot(startFrames/10, individual_pw_law[:, red_particle_idx], 'r-', zorder = 10)\n",
    "for i in range(nDrops):\n",
    "    if i != red_particle_idx:\n",
    "        ax.plot(startFrames/10, individual_pw_law[:, i], 'b-', zorder=0, alpha = 0.5, linewidth = 0.5)\n",
    "ax.set(xlabel = \"Window time [s]\", ylabel = r'$\\beta$', title = \"Droplet power law exponent by window time\")\n",
    "ax.grid()\n",
    "ax.legend([\"Red droplet\", \"Blue droplets\"])\n",
    "if save_verb: plt.savefig(res_path + \"/mean_squared_displacement/windowed_analysis/beta_wind_hough.png\", bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "ax.scatter(d_wind[:, i], individual_pw_law[:, red_particle_idx], s= 5, c = startFrames/10, marker = 'o', cmap = cm.Reds, zorder = 10)\n",
    "for i in range(nDrops):\n",
    "    if i != red_particle_idx:\n",
    "        scatter = ax.scatter(d_wind[:, i], individual_pw_law[:, i], s= 5, c = startFrames/10, marker = 'o', cmap = cm.Blues, zorder = 0)\n",
    "ax.set(ylabel = r\"$\\beta$\", xlabel = \"r [px]\", title = \"Windowed Power law exponent vs droplet radius\")\n",
    "ax.grid()\n",
    "fig.legend(*scatter.legend_elements(), title = \"Window Time\", fontsize = 8, loc=7)\n",
    "if save_verb: plt.savefig(res_path + \"/radius_scatterplots/beta_vs_radius_wind_hough.png\", bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "scatter = ax.scatter(mean_r_wind, ensemble_pw_lab_b, s= 5, c = startFrames/10, marker = 'o', cmap = cm.Blues)\n",
    "ax.scatter(mean_r_wind, ensemble_pw_lab_r, s= 5, c = startFrames/10, marker = 'o', cmap = cm.Reds)\n",
    "ax.set(ylabel = r\"$\\beta$\", xlabel = \"r [px]\", title = \"Windowed ensamble Power law exponent vs mean droplet radius\")\n",
    "ax.grid()\n",
    "fig.legend(*scatter.legend_elements(), title = \"Window Time\", fontsize = 8, loc=7)\n",
    "if save_verb: plt.savefig(res_path + \"/radius_scatterplots/beta_vs_mean_radius_wind_hough.png\", bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vs Velocity Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_verb = False\n",
    "run_windowed_analysis = True\n",
    "overwrite = False\n",
    "animated_plot_verb = False\n",
    "plot_verb = False\n",
    "\n",
    "print(f\"Velocity Autocorrelation Analysis: show_verb = {show_verb}, run_windowed_analysis = {run_windowed_analysis}, animated_plot_verb = {animated_plot_verb}\")\n",
    "%run ./analysis_modules/velocity_autocorrelation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize = (10, 5), sharex = True, sharey = True)\n",
    "scatter = ax.scatter(vacf_b_wind[\"0\"], mean_r_wind, s = 10, c = startFrames/10, marker = 'o', cmap = cm.Blues)\n",
    "scatter1 = ax.scatter(vacf_r_wind[\"0\"], mean_r_wind, s = 10, c = startFrames/10, marker = 'o', cmap = cm.Reds)\n",
    "ax.set(title = \"Raw Trajs\", xlabel = r\"$\\sigma_v^2$\", ylabel = \"d [px]\" )\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "\n",
    "scatter2 = ax1.scatter(vacf_b_wind_smooth[\"0\"], mean_r_wind, s = 10, c = startFrames/10, marker = 'o', cmap = cm.Blues)\n",
    "scatter3 = ax1.scatter(vacf_r_wind_smooth[\"0\"], mean_r_wind, s = 10, c = startFrames/10, marker = 'o', cmap = cm.Reds)\n",
    "ax1.set(title = \"Smooth Trajs\", xlabel = r\"$\\sigma_v^2$\", ylabel = \"d [px]\")\n",
    "ax1.grid(True, linestyle='-', color = '0.75')\n",
    "\n",
    "fig.suptitle(\"Velocity Variance vs. Mean Radius\")\n",
    "fig.legend(*scatter3.legend_elements(), title = \"Window Time\", fontsize = 8, loc = 7)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=1)\n",
    "if save_verb: plt.savefig(res_path + '/radius_scatterplots/v_variance.png', bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vs Effective Temperature from velocity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_verb = False\n",
    "animated_plot_verb = False\n",
    "plot_verb = False\n",
    "save_verb = False\n",
    "v_step = 10\n",
    "print(f\"Speed and Turning Angles Analysis: show_verb = {show_verb}, animated_plot_verb = {animated_plot_verb}\")\n",
    "%run ./analysis_modules/velocity_distr.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_T_speed_b = blue_fit_wind[:, 0]\n",
    "eff_T_speed_b_sigma = blue_fit_wind[:, 1]\n",
    "eff_T_speed_r = red_fit_wind[:, 0]\n",
    "eff_T_speed_r_sigma = red_fit_wind[:, 1]\n",
    "\n",
    "eff_T_speed_b_smooth = blue_fit_wind_smooth[:, 0]\n",
    "eff_T_speed_b_sigma_smooth = blue_fit_wind_smooth[:, 1]\n",
    "eff_T_speed_r_smooth = red_fit_wind_smooth[:, 0]\n",
    "eff_T_speed_r_sigma_smooth = red_fit_wind_smooth[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize = (10, 5), sharex = True, sharey = True)\n",
    "scatter = ax.scatter(eff_T_speed_b, mean_r_wind, s = 20, c = startFrames/10, marker = 'o', cmap = cm.Blues)\n",
    "scatter1 = ax.scatter(eff_T_speed_r, mean_r_wind, s = 20, c = startFrames/10, marker = 'o', cmap = cm.Reds)\n",
    "ax.set(title = \"Raw Trajs\", xlabel = r\"$T_{eff} \\; [??]$\", ylabel = \"d [px]\")\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "\n",
    "scatter2 = ax1.scatter(eff_T_speed_b_smooth, mean_r_wind, s = 20, c = startFrames/10, marker = 'o', cmap = cm.Blues)\n",
    "scatter3 = ax1.scatter(eff_T_speed_r_smooth, mean_r_wind, s = 20, c = startFrames/10, marker = 'o', cmap = cm.Reds)\n",
    "ax1.set(title = \"Smooth Trajs\", xlabel = r\"$T_{eff} \\; [??]$\", ylabel = \"d [px]\")\n",
    "ax1.grid(True, linestyle='-', color = '0.75')\n",
    "\n",
    "fig.suptitle(\"Effective Temperature vs. Mean Radius\")\n",
    "fig.legend(*scatter3.legend_elements(), title = \"Window Time\", fontsize = 8, loc=7)\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(right=1)\n",
    "plt.tight_layout()\n",
    "if save_verb: plt.savefig(res_path + '/radius_scatterplots/eff_T.png', bbox_inches='tight')\n",
    "if show_verb:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 body\n",
    "\n",
    "sigma direi di usare il diametro, per b direi [0 - 3 sigma] a step non superiori a 0.2 sigma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def compute_bin(x, bin_edges):\n",
    "    # assuming uniform bins for now\n",
    "    n = bin_edges.shape[0] - 1\n",
    "    a_min = bin_edges[0]\n",
    "    a_max = bin_edges[-1]\n",
    "    # special case to mirror NumPy behavior for last bin\n",
    "    if x == a_max:\n",
    "        return n - 1 # a_max always in last bin\n",
    "    bin = int(n * (x - a_min) / (a_max - a_min))\n",
    "    if bin < 0 or bin >= n:\n",
    "        return None\n",
    "    else:\n",
    "        return bin\n",
    "@njit\n",
    "def numba_histogram(a, bin_edges, weights):\n",
    "    hist = np.zeros((len(bin_edges)-1,), dtype=np.intp)\n",
    "    for ind, x in enumerate(a):\n",
    "        bin = compute_bin(x, bin_edges)\n",
    "        if bin is not None:\n",
    "            hist[int(bin)] += weights[ind]\n",
    "    return hist\n",
    "\n",
    "@njit\n",
    "def numba_mean_ax0(a):\n",
    "    res = []\n",
    "    for i in prange(a.shape[1]):\n",
    "        res.append(a[:, i].mean())\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "\n",
    "# this doesn't work\n",
    "@njit(parallel = True)\n",
    "def three_body_frame(coords, bList, sigma, hist_bins):\n",
    "    \"\"\"\n",
    "    Computes the three body distribution for a single frame\n",
    "    coords : array of shape (nDrops, 3)\n",
    "        The coordinates of the drops\n",
    "    bList : array of shape (nB)\n",
    "        The b values to compute the three body distribution for\n",
    "    sigma : float   \n",
    "        The standard deviation of the gaussian used to weight the three body distribution\n",
    "    hist_bins : array of shape (nBins)\n",
    "        The bins to compute the three body distribution for\n",
    "    Returns \n",
    "    ------- \n",
    "    mean_3_body : array of shape (nB, nBins)\n",
    "        The mean three body distribution for each b value\n",
    "    \"\"\"\n",
    "    res = np.ones((len(bList), len(hist_bins)-1))\n",
    "    three_body = np.ones((nDrops, len(hist_bins)-1))\n",
    "    angles = np.ones(int((nDrops-1)*(nDrops-2)/2))\n",
    "    gauss_weights = np.zeros(int((nDrops-1)*(nDrops-2)/2))\n",
    "    for b_ind in prange(len(bList)):\n",
    "        b = bList[b_ind]\n",
    "        for i in prange(nDrops):\n",
    "            count = 0\n",
    "            r_i = coords[i]\n",
    "            for j in range(nDrops):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                r_ij = coords[j] - r_i\n",
    "                for k in range(j+1, nDrops):\n",
    "                    if k == i:\n",
    "                        continue\n",
    "                    r_ik = coords[k] - r_i\n",
    "                    angles[count] = np.arccos(np.dot(r_ij, r_ik) / (np.linalg.norm(r_ij) * np.linalg.norm(r_ik)))\n",
    "                    gauss_weights[count] = np.exp(-0.5*((np.linalg.norm(r_ij)-b)/sigma)**2) * np.exp(-0.5*((np.linalg.norm(r_ik)-b)/sigma)**2)\n",
    "                    count += 1\n",
    "            three_body[i] = numba_histogram(angles, hist_bins, gauss_weights)\n",
    "        res[b_ind] = numba_mean_ax0(three_body)\n",
    "    return res\n",
    "\n",
    "@njit(parallel = True)\n",
    "def three_body_frame_modified(coords, bList, sigma, hist_bins):\n",
    "    angles = np.ones((len(bList), nDrops, int((nDrops-1)*(nDrops-2)/2)))\n",
    "    gauss_weights = np.ones((len(bList), nDrops, int((nDrops-1)*(nDrops-2)/2)))\n",
    "    for b_ind in prange(len(bList)):\n",
    "        b = bList[b_ind]\n",
    "        for i in range(nDrops):\n",
    "            count = 0\n",
    "            r_i = coords[i]\n",
    "            for j in range(nDrops):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                r_ij = coords[j] - r_i\n",
    "                for k in range(j+1, nDrops):\n",
    "                    if k == i:\n",
    "                        continue\n",
    "                    r_ik = coords[k] - r_i\n",
    "                    angles[b_ind, i, count] = np.arccos(np.dot(r_ij, r_ik) / (np.linalg.norm(r_ij) * np.linalg.norm(r_ik)))\n",
    "                    gauss_weights[b_ind, i, count] = np.exp(-0.5*((np.linalg.norm(r_ij)-b)/sigma)**2) * np.exp(-0.5*((np.linalg.norm(r_ik)-b)/sigma)**2)\n",
    "                    count += 1\n",
    "    return angles, gauss_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1 --> doesn't work, problem in three_body_frame\n",
    "if 0:\n",
    "    COORDS = np.array(rawTrajs.loc[:,[\"x\",\"y\"]])\n",
    "    sigma = 38\n",
    "    bList = np.arange(0, 3*sigma, .2)\n",
    "    print(\"bList length: \", bList.shape[0])\n",
    "    frames = np.arange(0, 30000, 1, dtype = int)\n",
    "    nFrames = len(frames)\n",
    "\n",
    "    hist_bins = np.arange(0, np.pi, np.pi/100)\n",
    "    mean_3_body = np.zeros((nFrames, len(bList), len(hist_bins)-1))\n",
    "\n",
    "    for frame_ind in tqdm(range(nFrames)):\n",
    "        mean_3_body[frame_ind] = three_body_frame(COORDS[frames[frame_ind]:frames[frame_ind]+50], bList, sigma, hist_bins)\n",
    "            \n",
    "    # save to txt frames, bList and hist bins\n",
    "    if 1:\n",
    "        np.savetxt(f\"./{analysis_data_path}/3_body/frames.txt\", frames)\n",
    "        np.savetxt(f\"./{analysis_data_path}/3_body/bList.txt\", bList)\n",
    "        np.savetxt(f\"./{analysis_data_path}/3_body/hist_bins.txt\", hist_bins)\n",
    "        for i, b in enumerate(bList):\n",
    "            temp = pd.DataFrame(mean_3_body[:, i])\n",
    "            temp.columns = [str(i) for i in temp.columns]\n",
    "            temp.to_parquet(f\"./{analysis_data_path}/3_body/mean_3_body_{b}.parquet\")\n",
    "else:\n",
    "    frames = np.loadtxt(f\"./{analysis_data_path}/3_body/frames.txt\").astype(int)\n",
    "    nFrames = len(frames)\n",
    "    bList = np.loadtxt(f\"./{analysis_data_path}/3_body/bList.txt\")#.astype(int)\n",
    "    hist_bins = np.loadtxt(f\"./{analysis_data_path}/3_body/hist_bins.txt\")\n",
    "    mean_3_body = np.zeros((nFrames, len(bList), len(hist_bins)-1))\n",
    "    for i, b in enumerate(bList):\n",
    "        mean_3_body[:, i] = pd.read_parquet(f\"./{analysis_data_path}/3_body/mean_3_body_{b}.parquet\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 2 --> seems to work with three_body_frame_modified function\n",
    "COORDS = np.array(rawTrajs.loc[:,[\"x\",\"y\"]])\n",
    "sigma = 38\n",
    "# for thest_3body.parquet\n",
    "#bList = np.arange(0, 3*sigma + 1, 3*sigma/50) # step 0.2 too little --> 448 hours to complete 30k frames\n",
    "\n",
    "# for thest_3body_2.parquet\n",
    "bList = np.arange(0, 300, 300/50) \n",
    "print(\"bList length: \", bList.shape[0])\n",
    "hist_bins = np.arange(0, np.pi, np.pi/100)\n",
    "hist_centers = (hist_bins[:-1] + hist_bins[1:])/2\n",
    "# frames = np.arange(0, 30000, 10) for test\n",
    "if 0:\n",
    "    counts = np.zeros((len(frames), len(bList), nDrops, len(hist_bins)-1))\n",
    "    for frame_ind in tqdm(range(len(frames))):\n",
    "        test_ang, test_weights = three_body_frame_modified(COORDS[frames[frame_ind]:frames[frame_ind]+50], bList, sigma, hist_bins)\n",
    "        for b_ind in range(len(bList)):\n",
    "            for i in range(nDrops):\n",
    "                counts[frame_ind, b_ind, i] = np.histogram(test_ang[b_ind, i], bins = hist_bins, weights = test_weights[b_ind, i])[0]\n",
    "    test_reshape = counts.reshape((len(bList)*nDrops*(len(hist_bins)-1), len(frames)))\n",
    "    test_df = pd.DataFrame(test_reshape)\n",
    "    test_df.columns = frames.astype(str)\n",
    "    test_df.to_parquet(f\"./{analysis_data_path}/3_body/test_3body.parquet\")\n",
    "else:\n",
    "    try: \n",
    "        print(\"Importing data...\")\n",
    "        test_df = pd.read_parquet(f\"./{analysis_data_path}/3_body/test_3body2.parquet\")\n",
    "        frames = test_df.columns.astype(int)\n",
    "        counts = np.array(test_df).reshape((len(frames), len(bList), nDrops, len(hist_bins)-1))\n",
    "        mean_3_body = np.mean(counts, axis = 2)\n",
    "        mean_3_body_norm = mean_3_body / mean_3_body.sum(axis=2).reshape(mean_3_body.shape[0], mean_3_body.shape[1], 1)\n",
    "    except:\n",
    "        raise Exception(\"File not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in range(0, len(frames), 30):\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "    anim_running = True\n",
    "    def onClick(event):\n",
    "        global anim_running\n",
    "        if anim_running:\n",
    "            ani.event_source.stop()\n",
    "            anim_running = False\n",
    "        else:\n",
    "            ani.event_source.start()\n",
    "            anim_running = True\n",
    "\n",
    "    plot = ax.plot(hist_centers, mean_3_body_norm[frame, 0], color = \"black\")[0]\n",
    "    ax.set(ylabel = \"pdf\", xlabel = r\"$\\theta$\")\n",
    "    title = ax.set_title(f\"Three body metric at frame = {frame}, b: {np.round(bList[0],2)}\")\n",
    "\n",
    "    def animate_func(i):\n",
    "        plot.set_ydata(mean_3_body_norm[frame, i])\n",
    "        title.set_text(f\"Three body metric at frame = {frame}, b: {np.round(bList[i],2)}\")\n",
    "        return plot\n",
    "\n",
    "    fig.canvas.mpl_connect('button_press_event', onClick)\n",
    "    ani = matplotlib.animation.FuncAnimation(fig, animate_func, mean_3_body_norm.shape[1], interval = 300)\n",
    "    ani.save(f\"./hough_results/3body/test2/frame{frames[frame]}.mp4\", fps = 5)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "frame = 10\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "im = ax.imshow(counts[frame])\n",
    "ax.set(ylabel=\"b\", xlabel=\"angle\", title=f\"frame {frame}\")\n",
    "ax.set_xticklabels([f\"$\\pi$ /{i}\" for i in range(10)])\n",
    "ax.set_yticklabels(bList.astype(int))\n",
    "fig.colorbar(im, ax=ax)\n",
    "ax.set_aspect(1.8)\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (5, 5))\n",
    "anim_running = True\n",
    "def onClick(event):\n",
    "    global anim_running\n",
    "    if anim_running:\n",
    "        ani.event_source.stop()\n",
    "        anim_running = False\n",
    "    else:\n",
    "        ani.event_source.start()\n",
    "        anim_running = True\n",
    "\n",
    "im = ax.imshow(mean_3_body_norm[0])\n",
    "ax.set(ylabel = \"b\", xlabel = \"angle\")\n",
    "title = ax.set_title(f\"frame: {0}\")\n",
    "\n",
    "ax.set_xticks(np.arange(0, len(hist_bins), 10))\n",
    "ax.set_xticklabels(np.round(hist_bins,1)[::10])\n",
    "ax.set_yticks(np.arange(0, len(bList), 10))\n",
    "ax.set_yticklabels(bList.astype(int)[::10])\n",
    "ax.set_aspect(2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "def animate_func(i):\n",
    "    im.set_array(mean_3_body_norm[i])\n",
    "    title.set_text(f\"frame: {frames[i]}\")\n",
    "    return [im]\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', onClick)\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate_func, mean_3_body_norm.shape[0], interval = 100)\n",
    "ani.save(f'./{res_path}/3body/3_body_mean_test_norm.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\n",
    "plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUMULATIVE TURNING ANGLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nFrames = max(smoothTrajs.frame)\n",
    "\n",
    "blueTrajs, redTraj_smooth = get_trajs(nDrops, red_particle_idx, smoothTrajs)\n",
    "theta_blue = np.zeros((nDrops-1, nFrames-1))\n",
    "for i in range(nDrops-1):\n",
    "    theta_blue[i] = ys.turning_angles_ensemble([blueTrajs[i]], centered = True, accumulate = False)\n",
    "theta_red = ys.turning_angles_ensemble(redTraj, centered = True, accumulate = False)\n",
    "\n",
    "# cumulative turning angles\n",
    "theta_blue_cum = np.cumsum(theta_blue, axis=1)\n",
    "theta_red_cum = np.cumsum(theta_red)\n",
    "\n",
    "fig, (ax,ax1) = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "ax.plot(np.arange(0, nFrames-1)/10, np.mean(theta_blue_cum, axis = 0), 'b-', label = \"Blue droplets\")\n",
    "ax.set(title = \"Cumulative turning angles\", ylabel = r\"$\\theta$ [rad]\")\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "ax.legend(loc = \"upper left\")\n",
    "ax1.plot(np.arange(0, nFrames-1)/10, theta_red_cum, 'r-', label = \"Red droplet\")\n",
    "ax1.set(xlabel = \"Time [s]\", ylabel = r\"$\\theta$ [rad]\")\n",
    "ax1.grid(True, linestyle='-', color = '0.75')\n",
    "ax1.legend(loc = \"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"./{res_path}/turning_angles/cumulative_turning_angles.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_legend_handles_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5), sharex=True)\n",
    "blue = ax.plot(np.arange(0, nFrames-1)/10, theta_blue_cum.T, linewidth = 0.5, alpha = 0.5, color = 'b')\n",
    "red = ax.plot(np.arange(0, nFrames-1)/10, theta_red_cum, 'r-')\n",
    "ax.set(title = \"Individual cumulative turning angles\", ylabel = r\"$\\theta$ [rad]\", xlabel = \"Time [s]\")\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "plt.savefig(f\"./{res_path}/turning_angles/individual_cumulative_turning_angles.png\", bbox_inches = \"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blueTrajs, redTraj_smooth = get_trajs(nDrops, red_particle_idx, smoothTrajs)\n",
    "theta_blue = np.zeros((nDrops-1, nFrames))\n",
    "for i in range(nDrops-1):\n",
    "    theta_blue[i] = ys.turning_angles_ensemble([blueTrajs[i]], centered = True, accumulate = True)\n",
    "theta_red = ys.turning_angles_ensemble(redTraj, centered = True, accumulate = True)\n",
    "\n",
    "fig, (ax,ax1) = plt.subplots(2, 1, figsize=(10, 5), sharex=True)\n",
    "ax.plot(np.arange(0, nFrames)/10, np.mean(theta_blue, axis = 0), 'b-', label = \"Blue droplets\")\n",
    "ax.set(title = \"Cumulative turning angles - with accumulate = True\", ylabel = r\"$\\theta$ [rad]\")\n",
    "ax.grid(True, linestyle='-', color = '0.75')\n",
    "ax.legend(loc = \"upper left\")\n",
    "ax1.plot(np.arange(0, nFrames)/10, theta_red, 'r-', label = \"Red droplet\")\n",
    "ax1.set(xlabel = \"Time [s]\", ylabel = r\"$\\theta$ [rad]\")\n",
    "ax1.grid(True, linestyle='-', color = '0.75')\n",
    "ax1.legend(loc = \"upper left\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MEAN SQUARED ANGULAR DIFFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_msad(maxLagtime, r):\n",
    "    current_msd = np.empty(maxLagtime)\n",
    "    for lag_ in prange(1, maxLagtime + 1):\n",
    "        x_ = np.sum(r[lag_:] * r[:-lag_], axis=1) / (np.linalg.norm(r[lag_:], axis=1)*np.linalg.norm(r[:-lag_], axis=1))\n",
    "        temp = np.arccos( np.clip(x_, -1, 1) )\n",
    "        current_msd[lag_ - 1] = np.mean(temp)\n",
    "    return current_msd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean squared angular displacement \n",
    "blueTrajs, redTraj = get_trajs(nDrops, red_particle_idx, rawTrajs)\n",
    "maxLagtime = 1000\n",
    "msad = np.empty((len(blueTrajs), maxLagtime))\n",
    "for i in tqdm(range(len(blueTrajs))):\n",
    "    r = np.array(blueTrajs[i].r)\n",
    "    current_msd = get_msad(maxLagtime, r)\n",
    "    msad[i] = current_msd\n",
    "msad_b = np.mean(msad, axis=0)\n",
    "msad_b_std = np.std(msad, axis=0)\n",
    "\n",
    "msad_r = get_msad(maxLagtime, np.array(redTraj[0].r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msad_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1, maxLagtime+1, 1)/10, msad_b, 'b', label=\"blue\")\n",
    "ax.plot(np.arange(1, maxLagtime+1, 1)/10, msad_r, 'r', label=\"red\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax.set(xlabel=\"lag [s]\", ylabel=\"MSAD \", xscale=\"log\", yscale=\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VELOCITY CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit(parallel=True, fastmath=True)\n",
    "def get_corr(velocities, maxLagtime_):\n",
    "    xx = np.zeros((velocities.shape[0], maxLagtime_))\n",
    "    yy = np.zeros((velocities.shape[0], maxLagtime_))\n",
    "    xy = np.zeros((velocities.shape[0], maxLagtime_))\n",
    "    for i in prange(velocities.shape[0]):\n",
    "        v = velocities[i]\n",
    "        xx[i, 0] = np.corrcoef(v[0], v[0])[0, 1]\n",
    "        yy[i, 0] = np.corrcoef(v[1], v[1])[0, 1]\n",
    "        xy[i, 0] = np.corrcoef(v[0], v[1])[0, 1]\n",
    "        \n",
    "        for lag_ in prange(1, maxLagtime_):\n",
    "            xx[i, lag_] = np.corrcoef(v[0, :-lag_], v[0, lag_:])[0, 1]\n",
    "            yy[i, lag_] = np.corrcoef(v[1, :-lag_], v[1, lag_:])[0, 1]\n",
    "            xy[i, lag_] = np.corrcoef(v[0, :-lag_], v[1, lag_:])[0, 1]\n",
    "    return xx, yy, xy\n",
    "\n",
    "def corr_windowed(nSteps, maxLagtime, startFrames, endFrames, trajectories, red_particle_idx, nDrops):\n",
    "    corr_b = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_std_b = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_r = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_std_r = np.zeros((nSteps, 3, maxLagtime))\n",
    "\n",
    "    for k in tqdm(range(nSteps)):\n",
    "        trajs = trajectories.loc[trajectories.frame.between(startFrames[k], endFrames[k])]\n",
    "        blueTrajs, redTraj = get_trajs(nDrops, red_particle_idx, trajs)\n",
    "\n",
    "        res = np.array(get_corr(get_velocities(blueTrajs), maxLagtime))\n",
    "        corr_b[k], corr_std_b[k] = res.mean(axis=1), res.std(axis=1)\n",
    "        \n",
    "        res = np.array(get_corr(get_velocities(redTraj), maxLagtime))\n",
    "        corr_r[k], corr_std_r[k] = res.mean(axis=1), res.std(axis=1)\n",
    "\n",
    "    return corr_b, corr_std_b, corr_r, corr_std_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    maxLagtime = 300\n",
    "    corr_b, corr_std_b, corr_r, corr_std_r = corr_windowed(nSteps, maxLagtime, startFrames, endFrames, rawTrajs, red_particle_idx, nDrops)\n",
    "\n",
    "    windLenght = 10\n",
    "    polyOrder = 2\n",
    "    smoothTrajs = get_smooth_trajs(rawTrajs, nDrops, windLenght, polyOrder)\n",
    "    corr_b_smooth, corr_std_b_smooth, corr_r_smooth, corr_std_r_smooth = corr_windowed(nSteps, maxLagtime, startFrames, endFrames, smoothTrajs, red_particle_idx, nDrops)\n",
    "    if 1:\n",
    "        path = f\"./{analysis_data_path}/corr/raw/\"\n",
    "        pd.DataFrame(corr_b[:,0]).to_csv(path+\"corr_b_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_b[:,1]).to_csv(path+\"corr_b_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_b[:,2]).to_csv(path+\"corr_b_xy.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_b[:,0]).to_csv(path+\"corr_std_b_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_b[:,1]).to_csv(path+\"corr_std_b_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_b[:,2]).to_csv(path+\"corr_std_b_xy.csv\", index=False)\n",
    "        pd.DataFrame(corr_r[:,0]).to_csv(path+\"corr_r_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_r[:,1]).to_csv(path+\"corr_r_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_r[:,2]).to_csv(path+\"corr_r_xy.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_r[:,0]).to_csv(path+\"corr_std_r_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_r[:,1]).to_csv(path+\"corr_std_r_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_r[:,2]).to_csv(path+\"corr_std_r_xy.csv\", index=False)\n",
    "\n",
    "        path = f\"./{analysis_data_path}/corr/smooth/\"\n",
    "        text_file = open(path+\"specs.txt\", \"w\")\n",
    "        n = text_file.write(f'Window size: {windLenght}, polyorder: {polyOrder}')\n",
    "        text_file.close()\n",
    "\n",
    "        pd.DataFrame(corr_b_smooth[:,0]).to_csv(path+\"corr_b_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_b_smooth[:,1]).to_csv(path+\"corr_b_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_b_smooth[:,2]).to_csv(path+\"corr_b_xy.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_b_smooth[:,0]).to_csv(path+\"corr_std_b_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_b_smooth[:,1]).to_csv(path+\"corr_std_b_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_b_smooth[:,2]).to_csv(path+\"corr_std_b_xy.csv\", index=False)\n",
    "        pd.DataFrame(corr_r_smooth[:,0]).to_csv(path+\"corr_r_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_r_smooth[:,1]).to_csv(path+\"corr_r_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_r_smooth[:,2]).to_csv(path+\"corr_r_xy.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_r_smooth[:,0]).to_csv(path+\"corr_std_r_x.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_r_smooth[:,1]).to_csv(path+\"corr_std_r_y.csv\", index=False)\n",
    "        pd.DataFrame(corr_std_r_smooth[:,2]).to_csv(path+\"corr_std_r_xy.csv\", index=False)\n",
    "else:\n",
    "    maxLagtime = 300\n",
    "    type_list = [\"x\", \"y\", \"xy\"]\n",
    "    path = f\"./{analysis_data_path}/corr/raw/\"\n",
    "    corr_b = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_b_std = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_r = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_r_std = np.zeros((nSteps, 3, maxLagtime))\n",
    "\n",
    "    for i in range(3):\n",
    "        corr_b[:, i] = pd.read_csv(path+f\"corr_b_{type_list[i]}.csv\").values\n",
    "        corr_b_std[:, i] = pd.read_csv(path+f\"corr_std_b_{type_list[i]}.csv\").values\n",
    "        corr_r[:, i] = pd.read_csv(path+f\"corr_r_{type_list[i]}.csv\").values\n",
    "        corr_r_std[:, i] = pd.read_csv(path+f\"corr_std_r_{type_list[i]}.csv\").values\n",
    "\n",
    "    path = f\"./{analysis_data_path}/corr/smooth/\"\n",
    "    corr_b_smooth = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_b_std_smooth = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_r_smooth = np.zeros((nSteps, 3, maxLagtime))\n",
    "    corr_r_std_smooth = np.zeros((nSteps, 3, maxLagtime))\n",
    "    for i in range(3):\n",
    "        corr_b_smooth[:, i] = pd.read_csv(path+f\"corr_b_{type_list[i]}.csv\").values\n",
    "        corr_b_std_smooth[:, i] = pd.read_csv(path+f\"corr_std_b_{type_list[i]}.csv\").values\n",
    "        corr_r_smooth[:, i] = pd.read_csv(path+f\"corr_r_{type_list[i]}.csv\").values\n",
    "        corr_r_std_smooth[:, i] = pd.read_csv(path+f\"corr_std_r_{type_list[i]}.csv\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type = \"Raw\"\n",
    "\n",
    "if type == \"Raw\":\n",
    "    corr_b_plot = corr_b\n",
    "    corr_r_plot = corr_r\n",
    "\n",
    "elif type == \"Smooth\":\n",
    "    corr_b_plot = corr_b_smooth\n",
    "    corr_r_plot = corr_r_smooth\n",
    "\n",
    "else:\n",
    "    print(\"Wrong type\")\n",
    "\n",
    "coord_list = [\"x\", \"y\", \"xy\"]\n",
    "\n",
    "fig = plt.figure(figsize = (8, 5))\n",
    "anim_running = True\n",
    "\n",
    "def onClick(event):\n",
    "    global anim_running\n",
    "    if anim_running:\n",
    "        ani.event_source.stop()\n",
    "        anim_running = False\n",
    "    else:\n",
    "        ani.event_source.start()\n",
    "        anim_running = True\n",
    "\n",
    "def update_graph(step):\n",
    "    title.set_text(f\"Velocity autocorrelation - {type} Trajectories - window [{startFrames[step]/10} - {endFrames[step]/10}] s\")\n",
    "    for i in range(3):\n",
    "        graphs[i].set_ydata(corr_b_plot[step, i])  \n",
    "        graphs1[i].set_ydata(corr_b_plot[step, i])\n",
    "    return graphs, graphs1\n",
    "\n",
    "ax = fig.add_subplot(211)\n",
    "title = ax.set_title(f\"Velocity autocorrelation - {type} Trajectories - window [{startFrames[0]/10} - {endFrames[0]/10}] s\")\n",
    "graphs = []\n",
    "for i in range(3):\n",
    "    graphs.append(ax.plot(np.arange(0, maxLagtime, 1)/10, corr_b_plot[0, i], label = f\"{coord_list[i]}\")[0])\n",
    "ax.set(ylabel = r'vacf [$(px/s)^2$]', xlabel = 'lag time $t$ [s]', xlim = (-1, 10), ylim = (-0.5, 1))\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax1 = fig.add_subplot(212)\n",
    "graphs1 = []\n",
    "for i in range(3):\n",
    "    graphs1.append(ax1.plot(np.arange(0, maxLagtime, 1)/10, corr_r_smooth[0, i], label = f\"{type_list[i]}\")[0])\n",
    "ax1.set(ylabel = r'vacf [$(px/s)^2$]', xlabel = 'lag time $t$ [s]', xlim = (-1, 10), ylim = (-0.5, 1))\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.canvas.mpl_connect('button_press_event', onClick)\n",
    "ani = matplotlib.animation.FuncAnimation(fig, update_graph, nSteps, interval = 5, blit=False)\n",
    "if 1:\n",
    "    ani.save(f'./{res_path}/corr/corr_{type.lower()}.mp4', fps=30, extra_args=['-vcodec', 'libx264'])\n",
    "if 0:\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS OF WIND PARAMETER ON FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_borders = np.arange(0, 100, .2)\n",
    "bin_centers = np.arange(0, 100, .2)[:-1] + .2 / 2\n",
    "bin_borders_turn = np.arange(-np.pi, np.pi + 0.0001, np.pi/50)\n",
    "bin_centers_turn = bin_borders_turn[:-1] + np.diff(bin_borders_turn) / 2\n",
    "v_step = 10\n",
    "\n",
    "windLenList = np.arange(3, 50, 1)\n",
    "v_b = np.zeros((len(windLenList), len(bin_centers)))\n",
    "v_r = np.zeros((len(windLenList), len(bin_centers)))\n",
    "t_b = np.zeros((len(windLenList), len(bin_centers_turn)))\n",
    "t_r = np.zeros((len(windLenList), len(bin_centers_turn)))\n",
    "for i in tqdm(range(len((windLenList)))):\n",
    "    traj = get_smooth_trajs(rawTrajs, nDrops, windLenList[i], 2)\n",
    "    blue, red = get_trajs(nDrops, red_particle_idx, traj) \n",
    "    v_b[i] = np.histogram(ys.speed_ensemble(blue, step = v_step), bin_borders, density=True)[0]\n",
    "    v_r[i] = np.histogram(ys.speed_ensemble(red, step = v_step), bin_borders, density=True)[0]\n",
    "    t_b[i] = np.histogram(ys.turning_angles_ensemble(blue, centered = True), bin_borders_turn, density=True)[0]\n",
    "    t_r[i] = np.histogram(ys.turning_angles_ensemble(red, centered = True), bin_borders_turn, density=True)[0]\n",
    "\n",
    "v_diff_b = np.zeros(len(windLenList) - 1)\n",
    "v_diff_r = np.zeros(len(windLenList) - 1)\n",
    "t_diff_b = np.zeros(len(windLenList) - 1)\n",
    "t_diff_r = np.zeros(len(windLenList) - 1)\n",
    "# mean absolute difference between two consecutive windows\n",
    "for i in range(len(windLenList)-1):\n",
    "    v_diff_b[i] = np.mean(np.abs(v_b[i] - v_b[i+1]))\n",
    "    v_diff_r[i] = np.mean(np.abs(v_r[i] - v_r[i+1]))\n",
    "    t_diff_b[i] = np.mean(np.abs(t_b[i] - t_b[i+1]))\n",
    "    t_diff_r[i] = np.mean(np.abs(t_r[i] - t_r[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (10, 4))\n",
    "anim_running = True\n",
    "def onClick(event):\n",
    "    global anim_running\n",
    "    if anim_running:\n",
    "        ani.event_source.stop()\n",
    "        anim_running = False\n",
    "    else:\n",
    "        ani.event_source.start()\n",
    "        anim_running = True\n",
    "\n",
    "def prepare_animation(hist):\n",
    "    def animate(i):\n",
    "        # simulate new data coming in\n",
    "        n = t_b[i]\n",
    "        for count, rect in zip(n, hist.patches):\n",
    "            rect.set_height(count)\n",
    "        title.set_text(f\"Turning angles distribution with window size = {windLenList[i]}\")\n",
    "        return hist.patches, title\n",
    "    \n",
    "    return animate\n",
    "\n",
    "\n",
    "hist = ax.bar(bin_centers_turn, t_b[0], width = np.diff(bin_borders_turn), color = \"blue\", alpha = 0.5, label = \"Blue\")\n",
    "title = ax.set_title(f\"Turning angles distribution with window size = {windLenList[0]}\")\n",
    "ax.set(xlabel=\"Turning angle\", ylabel=\"Probability density\")\n",
    "fig.canvas.mpl_connect('button_press_event', onClick)\n",
    "ani = matplotlib.animation.FuncAnimation(fig, prepare_animation(hist), 47, repeat=False, blit=False, interval=1000)\n",
    "ani.save(f'./{res_path}/smoothing_analysis/turning_angles_anim.mp4', fps=3, extra_args=['-vcodec', 'libx264'])\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize = (10, 4))\n",
    "ax.plot(windLenList[:-1], v_diff_b, label = 'blue')\n",
    "ax.plot(windLenList[:-1], v_diff_r, label = 'red')\n",
    "ax.set(xlabel = 'window length', title = 'mean difference in counts - velocity distribution')\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "ax1.plot(windLenList[:-1], t_diff_b, label = 'blue')\n",
    "ax1.plot(windLenList[:-1], t_diff_r, label = 'red')\n",
    "ax1.set(xlabel = 'window length', title = 'mean difference in counts - turning angles distribution')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'./{res_path}/smoothing_analysis/v_and_turn.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
