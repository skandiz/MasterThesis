{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('image', cmap='gray')\n",
    "mpl.rcParams[\"image.interpolation\"] = 'none'\n",
    "%matplotlib inline\n",
    "import matplotlib.animation\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from stardist import fill_label_holes, random_label_cmap, calculate_extents, gputools_available\n",
    "from stardist.matching import matching, matching_dataset\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "from csbdeep.utils import Path, normalize\n",
    "import skimage\n",
    "from tifffile import imsave, imread\n",
    "from glob import glob\n",
    "\n",
    "np.random.seed(42)\n",
    "lbl_cmap = random_label_cmap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUILD TRAIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame(cap, frame, x1, y1, x2, y2, w, h, preprocess):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
    "    ret, image = cap.read()\n",
    "    if preprocess:\n",
    "        npImage = np.array(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "        alpha = Image.new('L', (w, h), 0)\n",
    "        draw = ImageDraw.Draw(alpha)\n",
    "        draw.pieslice(((x1, y1), (x2, y2)), 0, 360, fill=255)\n",
    "        npAlpha = np.array(alpha)\n",
    "        npImage = npImage*npAlpha\n",
    "        ind = np.where(npImage == 0)\n",
    "        npImage[ind] = npImage[200, 200]\n",
    "        npImage = npImage[y1:y2, x1:x2]\n",
    "        return npImage\n",
    "    elif not preprocess:\n",
    "        return cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    else:\n",
    "        raise ValueError(\"preprocess must be a boolean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = pd.read_parquet('./49b_1r/old/tracking_data/hough/tracking_hough_trackpy_linking.parquet')\n",
    "# rename column to be consistent with the other datasets\n",
    "trajectories.rename(columns={'d':'r'}, inplace=True)\n",
    "# remove rows with NaN values in the radius column\n",
    "trajectories.dropna(subset=['r'], inplace=True)\n",
    "display(trajectories.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture('/Users/matteoscandola/MasterThesis/tracking/data/49b1r.mp4')\n",
    "video.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "_, first_frame = video.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample x frames from the hough-method tracking dataset\n",
    "n_samples = 1000\n",
    "np.random.seed(42)\n",
    "sample_frames = np.sort(np.random.choice(trajectories.frame.unique().astype(int), size=n_samples, replace=False))\n",
    "\n",
    "for frame in tqdm(sample_frames):\n",
    "    frame_img = get_frame(video, frame, 55, 55, 880, 880, 920, 960, True)\n",
    "    imsave(f\"/Users/matteoscandola/MasterThesis/tracking/train_49b-1r/image/49b1r_frame{frame}.tif\", frame_img)\n",
    "    df = trajectories.loc[(trajectories.frame == frame), [\"x\", \"y\", \"r\"]]\n",
    "    image = np.zeros((880 - 55, 880 - 55), dtype=np.uint8)\n",
    "    count = 0\n",
    "    for _, droplet in df.iterrows():\n",
    "        x, y, radius = droplet['x'], droplet['y'], droplet['r']\n",
    "        cv2.circle(image, (int(x)- 55, int(y) - 55), int(radius), int(count), thickness=-1)\n",
    "        count += 1\n",
    "    imsave(f\"/Users/matteoscandola/MasterThesis/tracking/train_49b-1r/mask/49b1r_frame{frame}.tif\", image)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUIL TRAIN_DATASET V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale_crop_and_resize(cap, frame, x1, y1, x2, y2, w, h):\n",
    "\tcap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
    "\tret, image = cap.read()\n",
    "\tnpImage = np.array(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\tnpImage = npImage[y1:y2, x1:x2]\n",
    "\treturn cv2.resize(npImage, (500, 500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './training_dataset/'\n",
    "nDrops = 50\n",
    "\n",
    "source_path_25b25r = \"./data/25b25r-1.mp4\" \n",
    "xmin_25b25r, ymin_25b25r, xmax_25b25r, ymax_25b25r = 95, 30, 535, 470\n",
    "video_25b25r = cv2.VideoCapture(source_path_25b25r)\n",
    "video_25b25r.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "w_25b25r = int(video_25b25r.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h_25b25r = int(video_25b25r.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_25b25r = int(video_25b25r.get(cv2.CAP_PROP_FPS))\n",
    "n_frames_25b25r = int(video_25b25r.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"25b25r video has {n_frames_25b25r} frames with a resolution of {w_25b25r}x{h_25b25r} and a framerate of {fps_25b25r} fps\")\n",
    "\n",
    "\n",
    "source_path_49b1r  = \"./data/49b1r.mp4\"\n",
    "xmin_49b1r,  ymin_49b1r,  xmax_49b1r,  ymax_49b1r  = 20, 50, 900, 930\n",
    "nDrops_post_merge_49b1r = 49\n",
    "video_49b1r = cv2.VideoCapture(source_path_49b1r)\n",
    "video_49b1r.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "w_49b1r = int(video_49b1r.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h_49b1r = int(video_49b1r.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps_49b1r = int(video_49b1r.get(cv2.CAP_PROP_FPS))\n",
    "n_frames_49b1r = int(video_49b1r.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(f\"49b1r video has {n_frames_49b1r} frames with a resolution of {w_49b1r}x{h_49b1r} and a framerate of {fps_49b1r} fps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"25b25r video can be cropped to : {ymax_25b25r - ymin_25b25r, xmax_25b25r - xmin_25b25r}\")\n",
    "print(f\"49b1r  video can be cropped to : {ymax_49b1r -  ymin_49b1r,  xmax_49b1r -  xmin_49b1r }\")\n",
    "\n",
    "start_49b1r = grayscale_crop_and_resize(video_49b1r, 0, xmin_49b1r, ymin_49b1r, xmax_49b1r, ymax_49b1r, w_49b1r, h_49b1r)\n",
    "end_49b1r = grayscale_crop_and_resize(video_49b1r, n_frames_49b1r-1, xmin_49b1r, ymin_49b1r, xmax_49b1r, ymax_49b1r, w_49b1r, h_49b1r)\n",
    "start_25b25r = grayscale_crop_and_resize(video_25b25r, 0, xmin_25b25r, ymin_25b25r, xmax_25b25r, ymax_25b25r, w_25b25r, h_25b25r)\n",
    "end_25b25r = grayscale_crop_and_resize(video_25b25r, n_frames_25b25r-1, xmin_25b25r, ymin_25b25r, xmax_25b25r, ymax_25b25r, w_25b25r, h_25b25r)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 8))\n",
    "ax[0,0].imshow(start_49b1r)\n",
    "ax[0,0].set_title('Start 49b1r')\n",
    "ax[0,1].imshow(end_49b1r)\n",
    "ax[0,1].set_title('End 49b1r')\n",
    "ax[1,0].imshow(start_25b25r)\n",
    "ax[1,0].set_title('Start 25b25r')\n",
    "ax[1,1].imshow(end_25b25r)\n",
    "ax[1,1].set_title('End 25b25r')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_samples_49b1r  = 100\n",
    "n_samples_25b25r = 100\n",
    "n_samples = n_samples_49b1r + n_samples_25b25r\n",
    "\n",
    "# 30000\n",
    "sample_frames_49b1r  = np.sort(np.array(random.sample(range(30000 ), n_samples_49b1r )), axis=0)\n",
    "sample_frames_25b25r = np.sort(np.array(random.sample(range(n_frames_25b25r), n_samples_25b25r)), axis=0)\n",
    "\n",
    "video_sample_49b1r = np.zeros((n_samples_49b1r, 500, 500))\n",
    "for i in tqdm(range(n_samples_49b1r)):\n",
    "    video_sample_49b1r[i] = grayscale_crop_and_resize(video_49b1r, sample_frames_49b1r[i], xmin_49b1r, ymin_49b1r, xmax_49b1r, ymax_49b1r, w_49b1r, h_49b1r)\n",
    "\n",
    "\n",
    "video_sample_25b25r = np.zeros((n_samples_25b25r, 500, 500))\n",
    "for i in tqdm(range(n_samples_25b25r)):\n",
    "    video_sample_25b25r[i] = grayscale_crop_and_resize(video_25b25r, sample_frames_25b25r[i], xmin_25b25r, ymin_25b25r, xmax_25b25r, ymax_25b25r, w_25b25r, h_25b25r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_hough(cap, frame, x1, y1, x2, y2, w, h):\n",
    "\tcap.set(cv2.CAP_PROP_POS_FRAMES, frame)\n",
    "\tret, image = cap.read()\n",
    "\tnpImage = np.array(cv2.cvtColor(image, cv2.COLOR_BGR2GRAY))\n",
    "\talpha = Image.new('L', (w, h), 0)\n",
    "\tdraw = ImageDraw.Draw(alpha)\n",
    "\tdraw.pieslice(((x1, y1), (x2, y2)), 0, 360, fill=255)\n",
    "\tnpAlpha = np.array(alpha)\n",
    "\tnpImage = npImage*npAlpha \n",
    "\tind = np.where(npImage == 0)\n",
    "\tnpImage[ind] = npImage[200, 200]\n",
    "\tnpImage = npImage[y1:y2, x1:x2]\n",
    "\treturn npImage #normalize(npImage)\n",
    "\t\n",
    "def hough_loc_frame(correct_n, frame, img, parameters):\n",
    "\tfound_circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT_ALT, **parameters)\n",
    "\tif found_circles is not None:\n",
    "\t\treturn np.hstack((found_circles[0], (np.ones((found_circles.shape[1], 1), dtype=int)*frame),\\\n",
    "\t\t\t\t\t\t\tnp.ones((found_circles.shape[1], 1), dtype=int)*found_circles.shape[1]))\n",
    "\telse:\n",
    "\t\treturn np.hstack((np.ones((1, 3))*-1, np.array([[frame, 0]])))\n",
    "\t\t\n",
    "def hough_feature_location(sample_frames, correct_n, params):\n",
    "\ttemp = []\n",
    "\tfor frame in tqdm(sample_frames):\n",
    "\t\timg = get_frame_hough(video, frame, xmin, ymin, xmax, ymax, w, h)\n",
    "\t\ttemp.append(hough_loc_frame(correct_n, frame, img, params))\n",
    "\t\n",
    "\ttemp_df = pd.DataFrame(np.concatenate([arr for arr in temp]), columns = [\"x\", \"y\", \"d\", \"frame\", \"nDroplets\"])\n",
    "\ttemp_df[\"frame\"] = temp_df[\"frame\"].astype(int)\n",
    "\ttemp_df[\"nDroplets\"] = temp_df[\"nDroplets\"].astype(int)\n",
    "\terr_frames = temp_df.loc[temp_df.nDroplets != correct_n].frame.unique().astype(int)\n",
    "\tloss = err_frames.shape[0]/sample_frames.shape[0]\n",
    "\treturn temp_df, err_frames, loss\n",
    "\n",
    "def optimize_params(x, *args):\n",
    "\tframes, correct_n = args\n",
    "\tparams = {\"dp\":x[0], \"minDist\":int(x[1]), \"param1\":x[2], \"param2\":x[3], \"minRadius\":int(x[4]), \"maxRadius\":int(x[5])}\n",
    "\terrs = 0\n",
    "\tfor i in tqdm(frames):\n",
    "\t\timg = get_frame_hough(video, i, xmin, ymin, xmax, ymax, w, h)\n",
    "\t\tfound_circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT_ALT, **params)\n",
    "\t\tif (found_circles is not None) and (found_circles.shape[1] == correct_n):\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\terrs += 1\n",
    "\t\t\t\n",
    "\tloss = errs/frames.shape[0]\n",
    "\ta = [loss, x[0], int(x[1]), x[2], x[3], int(x[4]), int(x[5])]\n",
    "\tprint(a)\n",
    "\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = {\"dp\":1.5, \"minDist\":3, \"param1\":50, \"param2\":0.8, \"minRadius\":3, \"maxRadius\":20}\n",
    "\n",
    "#temp_df, err_frames, loss = hough_feature_location(......."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted(glob(\"./train_49b-1r/image/*.tif\"))\n",
    "Y = sorted(glob(\"./train_49b-1r/mask/*.tif\"))\n",
    "assert all(Path(x).name==Path(y).name for x,y in zip(X,Y))\n",
    "\n",
    "X = list(map(imread,X))\n",
    "Y = list(map(imread,Y))\n",
    "n_channel = 1 if X[0].ndim == 2 else X[0].shape[-1]\n",
    "\n",
    "axis_norm = (0,1)   # normalize channels independently\n",
    "# axis_norm = (0,1,2) # normalize channels jointly\n",
    "if n_channel > 1:\n",
    "    print(\"Normalizing image channels %s.\" % ('jointly' if axis_norm is None or 2 in axis_norm else 'independently'))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "X = [normalize(x,1,99.8,axis=axis_norm) for x in tqdm(X)]\n",
    "Y = [fill_label_holes(y) for y in tqdm(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(X) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(X))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y[i] for i in ind_train] \n",
    "print('number of images: %3d' % len(X))\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img_label(img, lbl, img_title=\"image\", lbl_title=\"label\", **kwargs):\n",
    "    fig, (ai,al) = plt.subplots(1,2, figsize=(12,5), gridspec_kw=dict(width_ratios=(1.25,1)))\n",
    "    im = ai.imshow(img, cmap='gray', clim=(0,1))\n",
    "    ai.set_title(img_title)    \n",
    "    fig.colorbar(im, ax=ai)\n",
    "    al.imshow(lbl, cmap=lbl_cmap)\n",
    "    al.set_title(lbl_title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = min(9, len(X)-1)\n",
    "img, lbl = X[i], Y[i]\n",
    "assert img.ndim in (2,3)\n",
    "img = img if (img.ndim==2 or img.shape[-1]==3) else img[...,0]\n",
    "plot_img_label(img,lbl)\n",
    "None;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gputools_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 is a good default choice (see 1_data.ipynb)\n",
    "n_rays = 32\n",
    "\n",
    "# Use OpenCL-based computations for data generator during training (requires 'gputools')\n",
    "use_gpu = False and gputools_available()\n",
    "\n",
    "# Predict on subsampled grid for increased efficiency and larger field of view\n",
    "grid = (2,2)\n",
    "\n",
    "conf = Config2D (\n",
    "    n_rays       = n_rays,\n",
    "    grid         = grid,\n",
    "    use_gpu      = use_gpu,\n",
    "    n_channel_in = n_channel,\n",
    ")\n",
    "print(conf)\n",
    "vars(conf)\n",
    "\n",
    "if use_gpu:\n",
    "    from csbdeep.utils.tf import limit_gpu_memory\n",
    "    # adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations\n",
    "    limit_gpu_memory(0.8)\n",
    "    # alternatively, try this:\n",
    "    # limit_gpu_memory(None, allow_growth=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.time()\n",
    "model = StarDist2D(conf, name=f'stardist_{timestamp}', basedir='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_size = calculate_extents(list(Y), np.median)\n",
    "fov = np.array(model._axes_tile_overlap('YX'))\n",
    "print(f\"median object size:      {median_size}\")\n",
    "print(f\"network field of view :  {fov}\")\n",
    "if any(median_size > fov):\n",
    "    print(\"WARNING: median object size larger than field of view of the neural network.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_fliprot(img, mask): \n",
    "    assert img.ndim >= mask.ndim\n",
    "    axes = tuple(range(mask.ndim))\n",
    "    perm = tuple(np.random.permutation(axes))\n",
    "    img = img.transpose(perm + tuple(range(mask.ndim, img.ndim))) \n",
    "    mask = mask.transpose(perm) \n",
    "    for ax in axes: \n",
    "        if np.random.rand() > 0.5:\n",
    "            img = np.flip(img, axis=ax)\n",
    "            mask = np.flip(mask, axis=ax)\n",
    "    return img, mask \n",
    "\n",
    "def random_intensity_change(img):\n",
    "    img = img*np.random.uniform(0.6,2) + np.random.uniform(-0.2,0.2)\n",
    "    return img\n",
    "\n",
    "\n",
    "def augmenter(x, y):\n",
    "    \"\"\"Augmentation of a single input/label image pair.\n",
    "    x is an input image\n",
    "    y is the corresponding ground-truth label image\n",
    "    \"\"\"\n",
    "    x, y = random_fliprot(x, y)\n",
    "    x = random_intensity_change(x)\n",
    "    # add some gaussian noise\n",
    "    sig = 0.02*np.random.uniform(0,1)\n",
    "    x = x + sig*np.random.normal(0,1,x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some augmented examples\n",
    "img, lbl = X[0],Y[0]\n",
    "plot_img_label(img, lbl)\n",
    "for _ in range(3):\n",
    "    img_aug, lbl_aug = augmenter(img,lbl)\n",
    "    plot_img_label(img_aug, lbl_aug, img_title=\"image augmented\", lbl_title=\"label augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_demo = False\n",
    "\n",
    "if quick_demo:\n",
    "    print (\n",
    "        \"NOTE: This is only for a quick demonstration!\\n\"\n",
    "        \"      Please set the variable 'quick_demo = False' for proper (long) training.\",\n",
    "        file=sys.stderr, flush=True\n",
    "    )\n",
    "    model.train(X_trn, Y_trn, validation_data=(X_val,Y_val), augmenter=augmenter,\n",
    "                epochs=2, steps_per_epoch=10)\n",
    "\n",
    "    print(\"====> Stopping training and loading previously trained demo model from disk.\", file=sys.stderr, flush=True)\n",
    "    model = StarDist2D.from_pretrained('2D_demo')\n",
    "else:\n",
    "    model.train(X_trn, Y_trn, validation_data=(X_val,Y_val), augmenter=augmenter, epochs=150, steps_per_epoch=100)\n",
    "None;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
