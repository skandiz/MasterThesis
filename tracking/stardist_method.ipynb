{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow numpy pandas matplotlib h5py joblib pims scipy tqdm scikit-image csbdeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-08 18:22:00.759262: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model '2D_versatile_fluo' for 'StarDist2D'.\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.479071, nms_thresh=0.3.\n",
      "StarDist2D(2D_versatile_fluo): YXC → YXC\n",
      "├─ Directory: None\n",
      "└─ Config2D(n_dim=2, axes='YXC', n_channel_in=1, n_channel_out=33, train_checkpoint='weights_best.h5', train_checkpoint_last='weights_last.h5', train_checkpoint_epoch='weights_now.h5', n_rays=32, grid=(2, 2), backbone='unet', n_classes=None, unet_n_depth=3, unet_kernel_size=[3, 3], unet_n_filter_base=32, unet_n_conv_per_depth=2, unet_pool=[2, 2], unet_activation='relu', unet_last_activation='relu', unet_batch_norm=False, unet_dropout=0.0, unet_prefix='', net_conv_after_unet=128, net_input_shape=[None, None, 1], net_mask_shape=[None, None, 1], train_shape_completion=False, train_completion_crop=32, train_patch_size=[256, 256], train_background_reg=0.0001, train_foreground_only=0.9, train_sample_cache=True, train_dist_loss='mae', train_loss_weights=[1, 0.2], train_class_weights=(1, 1), train_epochs=800, train_steps_per_epoch=400, train_learning_rate=0.0003, train_batch_size=8, train_n_val_patches=None, train_tensorboard=True, train_reduce_lr={'factor': 0.5, 'patience': 80, 'min_delta': 0}, use_gpu=False)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.animation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib widget \n",
    "import h5py \n",
    "\n",
    "mpl.rc('image', cmap='gray')\n",
    "#import trackpy as tp\n",
    "#tp.quiet()\n",
    "\n",
    "import joblib \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, json\n",
    "import pims\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "from scipy.optimize import dual_annealing, linear_sum_assignment\n",
    "from scipy.spatial import distance_matrix\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "import skimage\n",
    "from csbdeep.utils import normalize\n",
    "from stardist.models import StarDist2D\n",
    "from stardist.data import test_image_nuclei_2d\n",
    "from stardist.plot import render_label\n",
    "\n",
    "from stardist import random_label_cmap, _draw_polygons, export_imagej_rois\n",
    "\n",
    "np.random.seed(6)\n",
    "lbl_cmap = random_label_cmap()\n",
    "# initialize model with versatile fluorescence pretrained weights\n",
    "model = StarDist2D.from_pretrained('2D_versatile_fluo')\n",
    "print(model)\n",
    "\n",
    "run_preprocessing_verb = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_preload(startFrame, endFrame, data_preload_path, dataset_name):\n",
    "    with h5py.File(data_preload_path, 'r') as f:\n",
    "        dataset = f[dataset_name]\n",
    "        frameImg = dataset[startFrame:endFrame]\n",
    "    return frameImg\n",
    "\n",
    "@pims.pipeline\n",
    "def preprocessing(image, w, h, x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Preprocessing function for the data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : pims.Frame\n",
    "        Frame of the video.\n",
    "    x1 : int\n",
    "        x coordinate of the top left corner of the ROI. (region of interest)\n",
    "    y1 : int\n",
    "        y coordinate of the top left corner of the ROI.\n",
    "    x2 : int    \n",
    "        x coordinate of the bottom right corner of the ROI.\n",
    "    y2 : int    \n",
    "        y coordinate of the bottom right corner of the ROI.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npImage : np.array\n",
    "        Preprocessed image.\n",
    "    \"\"\"\n",
    "    npImage = np.array(image)\n",
    "    alpha = Image.new('L', (h, w), 0)\n",
    "    draw = ImageDraw.Draw(alpha)\n",
    "    draw.pieslice(((x1, y1), (x2, y2)), 0, 360, fill=255)\n",
    "    npAlpha = np.array(alpha)\n",
    "    npImage = cv2.cvtColor(npImage, cv2.COLOR_BGR2GRAY)*npAlpha\n",
    "    ind = np.where(npImage == 0)\n",
    "    npImage[ind] = npImage[200, 200]\n",
    "    kernel = np.array([[0, -1, 0],\n",
    "                       [-1, 5,-1],\n",
    "                       [0, -1, 0]])\n",
    "    # sharpen image https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
    "    image_sharp = cv2.filter2D(src=npImage, ddepth=-1, kernel=kernel)\n",
    "    return image_sharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import data 25b_25r ...\n"
     ]
    }
   ],
   "source": [
    "video_selection = \"25b25r\"\n",
    "#video_selection = \"49b1r\"\n",
    "\n",
    "if video_selection == \"49b1r\":\n",
    "    print(\"Import data 49b_1r ...\")\n",
    "    system_name = \"49b-1r system\"\n",
    "    source_path = './data/49b1r.mp4'\n",
    "    path = './49b_1r/'\n",
    "    part = 1\n",
    "    data_preload_path = f'/Volumes/ExtremeSSD/UNI/h5_data_thesis/49b-1r/part{part}.h5'\n",
    "\n",
    "    ref = pims.open(source_path)\n",
    "    h = 920\n",
    "    w = 960\n",
    "    xmin = 55\n",
    "    ymin = 55\n",
    "    xmax = 880\n",
    "    ymax = 880\n",
    "    data  = preprocessing(ref, h, w, xmin, ymin, xmax, ymax) \n",
    "    frames = np.arange(0, 30000, 1)\n",
    "    fps = 10\n",
    "    nDrops = 50\n",
    "\n",
    "elif video_selection == \"25b25r\":\n",
    "    print(\"Import data 25b_25r ...\")\n",
    "    system_name = \"25b-25r system\"\n",
    "    source_path = './data/25b25r-1.mp4'\n",
    "    part = 2\n",
    "    path = f'./25b_25r/part{part}/'\n",
    "    data_preload_path = f'/Volumes/ExtremeSSD/UNI/h5_data_thesis/25b-25r/part{part}.h5'\n",
    "    ref = pims.open(source_path)\n",
    "    if part == 1:\n",
    "        rmax = 12\n",
    "        rmin = 8.3\n",
    "        frames = np.arange(0, 10**5, 1, dtype=int)\n",
    "    if part == 2:\n",
    "        rmax = 11\n",
    "        rmin = 7\n",
    "        frames = np.arange(10**5, 2*10**5, 1, dtype=int)\n",
    "    h = 480\n",
    "    w = 640\n",
    "    xmin = 100\n",
    "    ymin = 35 \n",
    "    xmax = 530\n",
    "    ymax = 465\n",
    "    data  = preprocessing(ref, h, w, xmin, ymin, xmax, ymax) \n",
    "    \n",
    "    fps = 30\n",
    "    nDrops = 50\n",
    "else:\n",
    "    raise ValueError(\"No valid video selection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Frames>\n",
       "Format: H.264 / AVC / MPEG-4 AVC / MPEG-4 part 10\n",
       "Source: ./data/25b25r-1.mp4\n",
       "Duration: 18000.000 seconds\n",
       "Frame rate: 30.000 fps\n",
       "Length: 540000 frames\n",
       "Frame Shape: (480, 640, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    #os.remove(data_preload_path)\n",
    "    hdf = h5py.File(data_preload_path, \"a\")\n",
    "    dset = hdf.create_dataset(name  = \"dataset_name\", shape = (0, data[0].shape[0], data[0].shape[1]),\n",
    "                              maxshape = (None, data[0].shape[0], data[0].shape[1]), dtype = data[0].dtype)\n",
    "    for frame in tqdm(frames):\n",
    "        N = 1\n",
    "        dset.resize(dset.shape[0] + N, axis=0)\n",
    "        new_data = data[frame]\n",
    "        dset[-N:] = new_data\n",
    "    hdf.close() \n",
    "else:\n",
    "    preprocessed_data = get_data_preload(frames[0]-frames[0], frames[10000]-frames[0], data_preload_path, 'dataset_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE LOCATION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = 10\n",
    "img = data[frame]\n",
    "labels_test, dict_test = model.predict_instances(normalize(img), predict_kwargs = {'verbose':False}) \n",
    "test = skimage.measure.regionprops_table(labels_test, properties=('centroid', 'area'))\n",
    "\n",
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize = (10,6), sharex=True, sharey=True)\n",
    "coord, points, prob = dict_test['coord'], dict_test['points'], dict_test['prob']\n",
    "\n",
    "ax.imshow(img, cmap='gray'); \n",
    "ax.set(title = 'Preprocessed Image', xlabel='X [px]', ylabel='Y [px]')\n",
    "ax1.imshow(img, cmap='gray'); \n",
    "_draw_polygons(coord, points, prob, show_dist=True)\n",
    "ax1.set(title = f\"Stardist result\", xlabel='X [px]', ylabel='Y [px]')\n",
    "plt.tight_layout()\n",
    "ax.set(xlim=(462, 510), ylim = (280, 330))\n",
    "plt.savefig(path + 'stardist_example.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST\n",
    "if 1:\n",
    "    droplets_found = []\n",
    "    area = []\n",
    "    frames_sample = np.random.choice(np.arange(0, preprocessed_data.shape[0], 1), 10, replace=False)\n",
    "\n",
    "    for frame in tqdm(frames_sample):\n",
    "        img = preprocessed_data[frame]\n",
    "        labels_test, dict_test = model.predict_instances(normalize(img), predict_kwargs = {'verbose':False}) \n",
    "        droplets_found.append(dict_test['coord'].shape[0])\n",
    "        test = skimage.measure.regionprops_table(labels_test, properties=('centroid', 'area'))\n",
    "        area += list(test['area'])\n",
    "        \"\"\"\n",
    "        plt.figure(figsize = (10, 5))\n",
    "        coord, points, prob = dict_test['coord'], dict_test['points'], dict_test['prob']\n",
    "        ax = plt.subplot(121)\n",
    "        ax.imshow(img, cmap='gray'); \n",
    "        ax.set(title = 'Preprocessed Image', xlabel='x', ylabel='y')\n",
    "        ax1 = plt.subplot(122, sharex=ax, sharey=ax)\n",
    "        ax1.imshow(img, cmap='gray'); \n",
    "        _draw_polygons(coord, points, prob, show_dist=True)\n",
    "        ax1.set(title = f\"Stardist result - {dict_test['coord'].shape[0]} droplets found\", xlabel='x', ylabel='y')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(path + f'test_samples/test{frame}.pdf', format='pdf')\n",
    "        plt.close()\n",
    "        \"\"\"\n",
    "print(droplets_found)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(np.sqrt(area)/np.sqrt(np.pi))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    print(\"Initialize model with versatile fluorescence pretrained weights...\")\n",
    "    model = StarDist2D.from_pretrained('2D_versatile_fluo')\n",
    "    print(model)\n",
    "    print(\"Starting location process...\") \n",
    "    area, x, y, prob, frames = [], [], [], [], []\n",
    "    start = timer()\n",
    "    for frame in frames:\n",
    "        segmented_image, dict_test = model.predict_instances(normalize(preprocessed_data[frame-framesList[0]]), \\\n",
    "                                                             predict_kwargs = {'verbose':False})\n",
    "        test = skimage.measure.regionprops_table(segmented_image, properties=('centroid', 'area'))\n",
    "        area += list(test['area'])\n",
    "        y += list(test['centroid-0'])\n",
    "        x += list(test['centroid-1'])\n",
    "        prob += list(dict_test['prob'])\n",
    "        frames += list(np.ones(len(list(test['centroid-0'])))*frame)\n",
    "    end = timer()\n",
    "    print(f\"Segmentation time: {end-start}\")\n",
    "\n",
    "    # save data\n",
    "    print(\"Saving data...\")\n",
    "    df = pd.DataFrame({'x':x, 'y':y, 'area':area, 'prob':prob, 'frame':frames})\n",
    "    df['frame'] = df.frame.astype('int')\n",
    "    df['r'] = np.sqrt(df.area/np.pi)\n",
    "    df.sort_values(by=['frame', 'prob'], ascending=[True, False], inplace=True)\n",
    "    df.to_parquet(path + f'df.parquet')\n",
    "else:\n",
    "    df = pd.read_parquet(path + 'raw_trajectories.parquet')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POST PROCESSING OF FEATURE LOCATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path + 'raw_trajectories.parquet')\n",
    "display(df.head())\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 4))\n",
    "ax[0, 0].plot(df.frame.unique(), df.groupby('frame').count().x.values)\n",
    "ax[0, 1].plot(df.r, '.')\n",
    "ax[1, 0].hist(df.area, bins=100, density=True)\n",
    "ax[1, 1].scatter(df.r, df.prob, s=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_frame = 21183#np.where(df.groupby('frame').count().x.values==60)[0][0] + df.frame.min()\n",
    "img = get_data_preload(selected_frame-df.frame.min(), selected_frame-df.frame.min()+1, data_preload_path, 'dataset_name').reshape(h, w)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.set_title(f\"Frame {selected_frame}\")\n",
    "ax.imshow(img, cmap='gray')\n",
    "for i in range(len(df.loc[df.frame == selected_frame])):\n",
    "    ax.add_artist(plt.Circle((df.loc[df.frame == selected_frame].x.values[i], df.loc[df.frame == selected_frame].y.values[i]), \\\n",
    "                                df.loc[df.frame == selected_frame].r.values[i], color='r', fill=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmax = 12\n",
    "rmin = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter found features\n",
    "if 1:\n",
    "    filtered_df = df.loc[df.r.between(rmin, rmax)]\n",
    "    filtered_df = filtered_df.groupby('frame').apply(lambda x: x.nlargest(nDrops, 'prob'))\n",
    "    filtered_df = filtered_df.reset_index(drop=True)\n",
    "\n",
    "print(\"frames:\", len(filtered_df.frame.unique()), \"errors:\", len(np.where(filtered_df.groupby('frame').count().x.values != nDrops)[0]))\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 4))\n",
    "ax[0, 0].plot(filtered_df.frame.unique(), filtered_df.groupby('frame').count().x.values)\n",
    "ax[0, 0].set(xlabel='frame', ylabel='droplets found')\n",
    "ax[0, 1].plot(filtered_df.r, '.')\n",
    "ax[0, 1].set(xlabel='droplet', ylabel='radius')\n",
    "ax[1, 0].hist(filtered_df.area, bins=100, density=True)\n",
    "ax[1, 0].set(xlabel='area', ylabel='density')\n",
    "ax[1, 1].scatter(filtered_df.r, filtered_df.prob, s=0.1)\n",
    "ax[1, 1].set(xlabel='radius', ylabel='probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_frames = np.where(filtered_df.groupby('frame').count().x != nDrops)[0] + filtered_df.frame.min()\n",
    "err_frame_select = np.random.choice(err_frames, 1)[0]\n",
    "img = get_data_preload(err_frame_select-df.frame.min(), err_frame_select-df.frame.min()+1, data_preload_path, 'dataset_name').reshape(h, w)\n",
    "\n",
    "print(len(err_frames), err_frames)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.set_title(f\"Frame {err_frame_select}\")\n",
    "ax.imshow(img, cmap='gray')\n",
    "for i in range(len(filtered_df.loc[filtered_df.frame == err_frame_select])):\n",
    "    ax.add_artist(plt.Circle((filtered_df.loc[filtered_df.frame == err_frame_select].x.values[i],\\\n",
    "                              filtered_df.loc[filtered_df.frame == err_frame_select].y.values[i]), \\\n",
    "                              filtered_df.loc[filtered_df.frame == err_frame_select].r.values[i], color='r', fill=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmmeh_frames = filtered_df.loc[filtered_df.r > 10.8].frame.unique()\n",
    "print(mmmeh_frames)\n",
    "frame = mmmeh_frames[0]\n",
    "df_plot = filtered_df.loc[filtered_df.frame == frame]\n",
    "\n",
    "img = get_data_preload(frame-filtered_df.frame.min(), frame-filtered_df.frame.min()+1, data_preload_path, 'dataset_name').reshape(h, w)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.imshow(img, cmap='gray')\n",
    "for i in range(len(df_plot)):\n",
    "    ax.add_artist(plt.Circle((df_plot.x.values[i], df_plot.y.values[i]), df_plot.r.values[i], color='r', fill=False))\n",
    "ax.set(title='Labeled elements', xlabel='x', ylabel='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRODUCTION OF TRAJECTORIES FROM REFINED FEATURE LOCATION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate missing values for each group\n",
    "interp_method = \"quadratic\"\n",
    "def interpolate_trajectory(group):\n",
    "    all_frames = pd.DataFrame({\"frame\": range(group[\"frame\"].min(), group[\"frame\"].max() + 1)})\n",
    "    merged = pd.merge(all_frames, group, on=\"frame\", how=\"left\")\n",
    "    merged.sort_values(by=\"frame\", inplace=True)\n",
    "    # Interpolate missing values\n",
    "    merged[\"x\"] = merged[\"x\"].interpolate(method = interp_method)\n",
    "    merged[\"y\"] = merged[\"y\"].interpolate(method = interp_method)\n",
    "    merged[\"r\"] = merged[\"r\"].interpolate(method = interp_method)\n",
    "    merged[\"area\"] = merged[\"area\"].interpolate(method = interp_method)\n",
    "    merged[\"prob\"] = merged[\"prob\"].interpolate(method = interp_method)\n",
    "    merged[\"frame\"] = merged[\"frame\"].interpolate(method = interp_method)\n",
    "    merged[\"particle\"].ffill(inplace=True)\n",
    "    merged[\"color\"].ffill(inplace=True)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################################\n",
    "#                                         LINK FEATURES WITH TRACKPY                                        #\n",
    "#############################################################################################################\n",
    "if 0:\n",
    "    cutoff = 100\n",
    "    mem = 5\n",
    "    t = tp.link_df(df, cutoff, memory = mem, link_strategy = 'hybrid', neighbor_strategy = 'KDTree', adaptive_stop = 1)\n",
    "    #print(t)\n",
    "    t = t.sort_values(['frame', 'particle'])\n",
    "\n",
    "    # filter out short trajectories (less than 25 frames)\n",
    "    t_filtered = tp.filter_stubs(t, 25)\n",
    "\n",
    "    # CREATE COLOR COLUMN AND SAVE DF\n",
    "    n = max(t_filtered.particle)\n",
    "    print(f\"N of droplets: { n + 1}\")\n",
    "    random.seed(5)\n",
    "    colors = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]) for i in range(n)]\n",
    "    for i in range(max(t_filtered.particle)+1-n):\n",
    "        colors.append(\"#00FFFF\")\n",
    "    c = []\n",
    "    for p in t.particle:\n",
    "        c.append(colors[p])\n",
    "    t[\"color\"] = c\n",
    "    #t.to_parquet(path + f'df_linked_raw_part{part}.parquet', index=False)\n",
    "\n",
    "    missing_droplets = {}\n",
    "    confront_array = np.arange(nDrops)\n",
    "    for err_frame in err_frames:\n",
    "        frame_df = t.loc[t.frame == err_frame]\n",
    "        missing_droplets[err_frame] = np.setdiff1d(confront_array, frame_df.particle.values)[0]\n",
    "    print(missing_droplets)\n",
    "    missing_droplets_df = pd.DataFrame.from_dict(missing_droplets, orient='index', columns=['particle'])\n",
    "    missing_droplets_df['frame'] = missing_droplets_df.index\n",
    "    missing_droplets_df.reset_index(drop = True, inplace = True)\n",
    "    #missing_droplets_df.to_parquet(path + f'missing_droplets_part{part}.parquet', index=False)\n",
    "\n",
    "    # fill missing positions, radii, areas and probabilities with interpolation (polynomial order 3)\n",
    "    trajectories = t.groupby(\"particle\").apply(interpolate_trajectory)\n",
    "    # Reset the index of the filled dataframe\n",
    "    trajectories.reset_index(drop=True, inplace=True)\n",
    "    trajectories[\"particle\"] = trajectories[\"particle\"].astype(int)\n",
    "    #trajectories.to_parquet(path + f'df_linked_part{part}_interpolated.parquet', index=False)\n",
    "else:\n",
    "    t = pd.read_parquet(path + 'df_linked_raw.parquet')\n",
    "    trajectories = pd.read_parquet(path + 'df_linked.parquet')\n",
    "    missing_droplets = pd.read_parquet(path + 'missing_droplets.parquet')\n",
    "    print(f\"Number of error frames: {len(missing_droplets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = t.groupby(\"particle\").apply(interpolate_trajectory)\n",
    "# Reset the index of the filled dataframe\n",
    "trajectories.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(t.groupby(\"frame\").size(), color = \"k\", label = \"n of droplets\")\n",
    "ax.axhline(nDrops, color = \"r\", linestyle = \"--\", label = \"correct n\")\n",
    "ax.set(xlabel = \"frame\", ylabel = \"n\")\n",
    "ax.set_title(\"Number of droplets per frame - pre interpolation\")\n",
    "ax.grid(linewidth = 0.2)\n",
    "ax.legend()\n",
    "#plt.savefig(path + 'n_of_droplets_raw.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 4))\n",
    "ax.plot(trajectories.groupby(\"frame\").size(), color = \"k\", label = \"n of droplets\")\n",
    "ax.axhline(nDrops, color = \"r\", linestyle = \"--\", label = \"correct n\")\n",
    "ax.set(xlabel = \"frame\", ylabel = \"n\")\n",
    "ax.set_title(\"Number of droplets per frame - after interpolation\")\n",
    "ax.grid(linewidth = 0.2)\n",
    "ax.legend()\n",
    "#plt.savefig(path + 'n_of_droplets.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "# example of interpolation\n",
    "val = 3\n",
    "raw = t.loc[(t.frame.between(missing_droplets.frame.values[val]-10, missing_droplets.frame.values[val]+10)) & (t.particle == missing_droplets.particle.values[val])]\n",
    "interpolated = trajectories.loc[(trajectories.frame.between(missing_droplets.frame.values[val]-10, missing_droplets.frame.values[val]+10)) & (trajectories.particle == missing_droplets.particle.values[val])]\n",
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize=(10, 4), sharey=True, sharex=True)\n",
    "ax.plot(raw.x, raw.y, '--bo')\n",
    "ax1.plot(interpolated.x, interpolated.y, '--ro')\n",
    "ax.set(title='Raw trajectory', xlabel = 'X [px]', ylabel = 'Y [px]')\n",
    "ax1.set(title='Interpolated trajectory', xlabel = 'X [px]', ylabel = 'Y [px]')\n",
    "plt.savefig(path + 'interpolation_example.png', bbox_inches='tight')\n",
    "ax.grid(linewidth = 0.2)\n",
    "ax1.grid(linewidth = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (5, 5))\n",
    "anim_running = True\n",
    "\n",
    "def onClick(event):\n",
    "    global anim_running\n",
    "    if anim_running:\n",
    "        ani.event_source.stop()\n",
    "        anim_running = False\n",
    "    else:\n",
    "        ani.event_source.start()\n",
    "        anim_running = True\n",
    "\n",
    "def update_graph(frame):\n",
    "    df = trajectory.loc[(trajectory.frame == frame) , [\"x\", \"y\", \"color\", \"r\"]]\n",
    "    for i in range(len(df)):\n",
    "        graph[i].center = (df.x.values[i], df.y.values[i])\n",
    "        graph[i].radius = df.r.values[i]\n",
    "    graph2.set_data(preprocessed_data[frame-trajectory.frame.min()])\n",
    "    title.set_text('Tracking raw - frame = {}'.format(frame))\n",
    "    return graph\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "title = ax.set_title('Tracking stardist + trackpy - frame = 0')\n",
    "ax.set(xlabel = 'X [px]', ylabel = 'Y [px]')\n",
    "df = trajectory.loc[(trajectory.frame == trajectory.frame.min()), [\"x\", \"y\", \"color\", \"r\"]]\n",
    "\n",
    "graph = []\n",
    "for i in range(len(df)):\n",
    "    graph.append(ax.add_artist(plt.Circle((df.x.values[i], df.y.values[i]), df.r.values[i], color = df.color.values[i],\\\n",
    "                                           fill = False, linewidth=1)))\n",
    "graph2 = ax.imshow(preprocessed_data[0])\n",
    "\n",
    "fig.canvas.mpl_connect('button_press_event', onClick)\n",
    "ani = matplotlib.animation.FuncAnimation(fig, update_graph, range(trajectory.frame.min(), trajectory.frame.max(), 1), interval = 5, blit=False)\n",
    "if 1: \n",
    "    writer = matplotlib.animation.FFMpegWriter(fps = 30, metadata = dict(artist='Matteo Scandola'), extra_args=['-vcodec', 'libx264'])\n",
    "    ani.save(path + 'tracking.mp4', writer=writer, dpi = 300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_parquet(path + 'df_linked_raw.parquet')\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 4))\n",
    "ax[0, 0].plot(filtered_df.frame.unique(), filtered_df.groupby('frame').count().x.values)\n",
    "ax[0, 0].set(xlabel='frame', ylabel='droplets found')\n",
    "ax[0, 1].plot(filtered_df.r, '.')\n",
    "ax[0, 1].set(xlabel='droplet', ylabel='radius')\n",
    "ax[1, 0].hist(filtered_df.area, bins=100, density=True)\n",
    "ax[1, 0].set(xlabel='area', ylabel='density')\n",
    "ax[1, 1].scatter(filtered_df.r, filtered_df.prob, s=0.1)\n",
    "ax[1, 1].set(xlabel='radius', ylabel='probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_parquet(path + 'df_linked.parquet')\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 4))\n",
    "ax[0, 0].plot(filtered_df.frame.unique(), filtered_df.groupby('frame').count().x.values)\n",
    "ax[0, 0].set(xlabel='frame', ylabel='droplets found')\n",
    "ax[0, 1].plot(filtered_df.r, '.')\n",
    "ax[0, 1].set(xlabel='droplet', ylabel='radius')\n",
    "ax[1, 0].hist(filtered_df.area, bins=100, density=True)\n",
    "ax[1, 0].set(xlabel='area', ylabel='density')\n",
    "ax[1, 1].scatter(filtered_df.r, filtered_df.prob, s=0.1)\n",
    "ax[1, 1].set(xlabel='radius', ylabel='probability')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
