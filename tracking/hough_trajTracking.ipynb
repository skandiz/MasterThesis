{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib ipympl\n",
    "plt.rcParams['figure.figsize'] = [10, 4]\n",
    "plt.rcParams['font.size'] = 8\n",
    "mpl.rc('image', cmap='gray')\n",
    "gs = gridspec.GridSpec(2, 2)\n",
    "\n",
    "import matplotlib.animation\n",
    "writervideo = matplotlib.animation.FFMpegWriter(fps=30)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv, json\n",
    "\n",
    "import pims\n",
    "import trackpy as tp\n",
    "tp.quiet()\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "\n",
    "from scipy.optimize import dual_annealing, minimize\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "import random\n",
    "\n",
    "run_analysis_verb = False\n",
    "show_verb = True\n",
    "save_verb = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@pims.pipeline\n",
    "def hough_preprocessing(image, x1, y1, x2, y2):    \n",
    "    #image = cv2.GaussianBlur(image, ksize = [7,7], sigmaX = 1.5, sigmaY = 1.5)\n",
    "    npImage = np.array(image)\n",
    "    # Create same size alpha layer with circle\n",
    "    alpha = Image.new('L', (920, 960), 0)\n",
    "\n",
    "    draw = ImageDraw.Draw(alpha)\n",
    "    draw.pieslice(((x1, y1), (x2, y2)), 0, 360, fill=255)\n",
    "\n",
    "    # Convert alpha Image to numpy array\n",
    "    npAlpha = np.array(alpha)\n",
    "    npImage = cv2.cvtColor(npImage, cv2.COLOR_BGR2GRAY)*npAlpha #npImage[:, :, 1] * npAlpha\n",
    "    \n",
    "    ind = np.where(npImage == 0)\n",
    "    # npImage[200, 200] color of the border to swap with the black\n",
    "    npImage[ind] = npImage[200, 200]\n",
    "    npImage = cv2.medianBlur(npImage, 3)\n",
    "    return npImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deprecated pixel format used, make sure you did set range correctly\n"
     ]
    }
   ],
   "source": [
    "# SETUP\n",
    "preload_load_data = False # takes 20 min\n",
    "merge_frame = 32269\n",
    "data = hough_preprocessing(pims.open('./data/movie.mp4'), 40, 55, 895, 910)\n",
    "if preload_load_data: \n",
    "    data_preload = list(data[:merge_frame])\n",
    "\n",
    "startFrame = 0\n",
    "endFrame = merge_frame\n",
    "frames = np.arange(startFrame, endFrame, 1)\n",
    "frames_opt = np.sort(random.sample(list(frames), 5000))\n",
    "correct_n = 50\n",
    "default_parameters = {\"dp\": 1.5, \"minDist\": 15, \"param1\": 100, \"param2\": 0.8, \"minRadius\": 15, \"maxRadius\": 25}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_frame(correct_n, frame, img, parameters):\n",
    "\tfound_circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT_ALT, **parameters)\n",
    "\tif (found_circles is not None) and (found_circles.shape[1] == correct_n):\n",
    "\t\treturn np.hstack((found_circles[0], (np.ones((correct_n, 1), dtype=int)*frame), np.ones((correct_n, 1), dtype=int)*correct_n))\n",
    "\telif (found_circles is not None) and (found_circles.shape[1] != correct_n):\n",
    "\t\treturn np.hstack((np.zeros((correct_n, 3)), (np.ones((correct_n, 1), dtype=int)*frame), np.ones((correct_n, 1), dtype=int)*found_circles.shape[1]))\n",
    "\telse:\n",
    "\t\treturn np.hstack((np.zeros((correct_n, 3)), (np.ones((correct_n, 1), dtype=int)*frame), np.zeros((correct_n, 1), dtype=int)))\n",
    "\n",
    "@joblib.delayed\n",
    "def loc_frame_parallel(correct_n, frame, img, parameters):\n",
    "\tfound_circles = cv2.HoughCircles(img, cv2.HOUGH_GRADIENT_ALT, **parameters)\n",
    "\tif (found_circles is not None) and (found_circles.shape[1] == correct_n):\n",
    "\t\treturn np.hstack((found_circles[0], (np.ones((correct_n, 1), dtype=int)*frame), np.ones((correct_n, 1), dtype=int)*correct_n))\n",
    "\telif (found_circles is not None) and (found_circles.shape[1] != correct_n):\n",
    "\t\treturn np.hstack((np.zeros((correct_n, 3)), (np.ones((correct_n, 1), dtype=int)*frame), np.ones((correct_n, 1), dtype=int)*found_circles.shape[1]))\n",
    "\telse:\n",
    "\t\treturn np.hstack((np.zeros((correct_n, 3)), (np.ones((correct_n, 1), dtype=int)*frame), np.zeros((correct_n, 1), dtype=int)))\n",
    "\n",
    "def hough_feature_location(data_preload, frames, correct_n, params, parallel_verb):\n",
    "    if parallel_verb:\n",
    "        parallel = joblib.Parallel(n_jobs = -1)\n",
    "        temp = parallel(\n",
    "            loc_frame_parallel(correct_n, frames[i], data_preload[i], params)\n",
    "            for i in range(len(frames)) #tqdm(range(len(frames)) )\n",
    "        )\n",
    "        #print(np.array(temp).shape)\n",
    "    else:\n",
    "        temp = []\n",
    "        for i in range(len(frames)):#tqdm(range(len(frames))):\n",
    "            temp.append(loc_frame(correct_n, frames[i], data_preload[i], params))\n",
    "        #print(np.array(temp).shape)\n",
    "    temp = pd.DataFrame(np.array(temp).reshape(len(frames)*correct_n, 5), columns = [\"x\", \"y\", \"d\", \"frame\", \"nDroplets\"])\n",
    "    err_frames = temp.loc[temp.nDroplets != correct_n].frame.unique().astype(int)\n",
    "    loss = err_frames.shape[0]/frames.shape[0]\n",
    "    return temp, err_frames, loss\n",
    "\n",
    "def optimize_params(x, *args):\n",
    "    data_preload, frames, correct_n = args\n",
    "    params = {\"dp\":x[0], \"minDist\":int(x[1]), \"param1\":x[2], \"param2\":x[3], \"minRadius\":int(x[4]), \"maxRadius\":int(x[5])}\n",
    "    _, _, loss = hough_feature_location(data_preload, frames, correct_n, params, False)\n",
    "    # Save the current best score and set of parameters to a CSV file\n",
    "    a = [loss, x[0], int(x[1]), x[2], x[3], int(x[4]), int(x[5])]\n",
    "    with open('optimization_results.csv', mode = 'a', newline='') as file:\n",
    "       writer = csv.writer(file)\n",
    "       writer.writerow(a)\n",
    "    print(a)\n",
    "    return loss\n",
    "\n",
    "def plot_optimization_results(opt_result_df, slot2):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.plot(np.arange(0, len(opt_result_df.loss), 1), opt_result_df.loss, 'b-')\n",
    "    ax.set_ylabel(\"loss\", color = 'b') \n",
    "    ax1 = ax.twinx() \n",
    "    ax1.plot(np.arange(0, len(opt_result_df[slot2]), 1), opt_result_df[slot2], 'r.')\n",
    "    ax1.set_ylabel(slot2, color='r')\n",
    "    ax.grid()\n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0014, 2.0, 8, 90.0, 0.8, 10, 35]\n",
      "[0.1592, 1.479039490222931, 13, 151.39343750476837, 0.7790394925718505, 11, 30]\n",
      "[0.0406, 1.2710235863924026, 14, 133.78300543874502, 0.5710235813034725, 11, 32]\n",
      "[0.9882, 2.5822854191064835, 12, 102.02556505613029, 0.3822854222973712, 10, 21]\n",
      "[0.383, 2.7896605785936117, 7, 172.0742613542825, 0.38966057969888873, 15, 31]\n",
      "[0.0108, 1.2978378981351852, 15, 88.58243867754936, 0.3978379145350556, 6, 27]\n",
      "[0.0368, 1.1067238869145513, 6, 132.39132466632873, 0.3067239038544422, 16, 31]\n",
      "[0.0336, 1.9494688361883163, 6, 132.39132466632873, 0.3067239038544422, 16, 31]\n",
      "[0.0336, 1.9494688361883163, 6, 132.39132466632873, 0.3067239038544422, 16, 31]\n",
      "[0.3388, 1.9494688361883163, 6, 167.50158191472292, 0.3067239038544422, 16, 31]\n",
      "[0.3392, 1.9494688361883163, 6, 167.50158191472292, 0.4197892867302319, 16, 31]\n"
     ]
    }
   ],
   "source": [
    "# Clear the contents of the CSV file before starting the optimization\n",
    "with open('optimization_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['loss', 'dp', 'minDist', 'param1', 'param1', 'minRadius', 'maxRadius'])\n",
    "\n",
    "opt_result = dual_annealing(optimize_params, x0 = [2, 8, 90, 0.8, 10, 35], args = (data_preload, frames_opt, correct_n),\\\n",
    "                            bounds = [(1, 3), (5, 20), (80, 200), (0.3, 1), (5, 20), (20, 40)], \\\n",
    "                            maxiter=10, seed=1234)\n",
    "\n",
    "opt_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_verb = True\n",
    "run_optimization_verb = True\n",
    "if optimization_verb:\n",
    "    if run_optimization_verb:\n",
    "        # Define a function to print the current best score and set of parameters\n",
    "        def callback(x, f, context):\n",
    "            print(f'Current score: {f}, Best parameters: {x}')\n",
    "            # Save the current best score and set of parameters to a CSV file\n",
    "            with open('optimization_results.csv', mode='a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(list(x) + [f])\n",
    "\n",
    "        # Clear the contents of the CSV file before starting the optimization\n",
    "        with open('optimization_results.csv', mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['dp', 'minDist', 'param1', 'param2', 'minRadius', 'maxRadius', 'loss'])\n",
    "        bounds = [(1, 3), (5, 20), (80, 200), (0.3, 1), (5, 20), (20, 40)]\n",
    "        opt_result = dual_annealing(optimize_params, x0 = [1.5, 15, 200, 0.9, 15, 25], args = (data_preload, frames_opt[:100], correct_n),\\\n",
    "                                    bounds = bounds, maxiter = 1000, callback=callback)\n",
    "    else:\n",
    "        try:\n",
    "            opt_result_df = pd.read_csv(\"./results/tracking_data/hough/pre_merge_optimization.csv\", sep=\"\\t\").sort_values(\"loss\", ascending=False)\n",
    "            opt_result_df = opt_result_df.sort_values(\"loss\", ascending=False)\n",
    "            optimized_parameters = opt_result_df.iloc[-1]\n",
    "            optimized_parameters = {\"dp\": optimized_parameters.dp, \"minDist\": optimized_parameters.minDist,\\\n",
    "                                    \"param1\": optimized_parameters.param1, \"param2\": optimized_parameters.param2,\\\n",
    "                                    \"minRadius\": int(optimized_parameters.minRadius), \"maxRadius\": int(optimized_parameters.maxRadius)}\n",
    "            parameters = optimized_parameters\n",
    "            print(\"Optimized parameters:\", parameters)\n",
    "            fig, ax = plt.subplots(1, 1, figsize = (10, 3))\n",
    "            ax.plot(opt_result_df.loss.values)\n",
    "            ax.set_ylabel(\"loss\")\n",
    "            ax.set_xlabel(\"iteration\")\n",
    "            ax.grid()\n",
    "            plt.show()\n",
    "        except:\n",
    "            raise Exception(\"No optimization results found\")\n",
    "else:\n",
    "    parameters = default_parameters\n",
    "    print(\"Default parameters:\", parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = 2.50993504   8.00993504 109.88808087   0.70993504  14.88808087 34.88808087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_err_frame(trial_f):\n",
    "    temp = cv2.HoughCircles(data_preload[trial_f], cv2.HOUGH_GRADIENT_ALT, **params)[0]\n",
    "\n",
    "    fig, (ax, ax1) = plt.subplots(1, 2, figsize = (10, 5))\n",
    "    ax.imshow(data_preload[trial_f])\n",
    "    ax.set_title(f\"Error frame: {trial_f} --> {temp.shape[0]} droplet found\")\n",
    "    for i in range(temp.shape[0]):\n",
    "        ax.add_artist(plt.Circle((temp[i, 0], temp[i, 1]), temp[i, 2], color='r', fill=False))\n",
    "    ax1.scatter(temp[:,2])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"dp\": .5, \"minDist\": 13, \"param1\": 80, \"param2\": 0.6, \"minRadius\": 11, \"maxRadius\": 29}\n",
    "pre_merge_df, err_frames, error = hough_feature_location(data_preload, frames_opt, correct_n, params, False)\n",
    "print(\"Error:\", error)\n",
    "print(\"Error frames:\", err_frames)\n",
    "if len(err_frames) > 0:\n",
    "    analyze_err_frame(err_frames[10])\n",
    "if len(err_frames) == 0:\n",
    "    pre_merge_df, err_frames, error = hough_feature_location(data_preload, frames, correct_n, params, False)\n",
    "    print(\"Error:\", error)\n",
    "    print(\"Error frames:\", err_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    # save to txt parameters:\n",
    "    with open('./results/tracking_data/hough/hough_pre_merge.txt', 'w') as f:\n",
    "        f.write(json.dumps(parameters))\n",
    "    pre_merge_df, err_frames, error = hough_feature_location(data_preload, frames, correct_n, parameters, False)\n",
    "    pre_merge_df.to_parquet(\"./results/tracking_data/hough_pre_merge.parquet\")\n",
    "else:\n",
    "    try:\n",
    "        parameters = json.load(open('./results/tracking_data/hough/hough_pre_merge.txt'))\n",
    "        pre_merge_df = pd.read_parquet(\"./results/tracking_data/hough/hough_pre_merge.parquet\")\n",
    "        print(parameters)\n",
    "        display(pre_merge_df)\n",
    "    except:\n",
    "        raise Exception(\"No pre merge data found, run analysis first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = (pre_merge_df.frame.values[::correct_n]).astype(int)\n",
    "print(f\"Number of frames: {len(frames)}\")\n",
    "err_frames = pre_merge_df.loc[pre_merge_df.nDroplets != correct_n].frame.unique().astype(int)\n",
    "print(f\"Percentage of error: {len(err_frames)}/{len(frames)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
